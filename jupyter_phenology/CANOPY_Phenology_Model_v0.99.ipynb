{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CANOPY-Phenology Model v0.99 in Jupyter Notebook format using Python v3.12**\n",
    "\n",
    "**The base phenology modeling algorithm was originally developed in 2018-2019 by:**\n",
    "\n",
    "Matthew Garcia, Ph.D.  \n",
    "Postdoctoral Research Associate  \n",
    "Dept. of Forest & Wildlife Ecology  \n",
    "University of Wisconsin - Madison  \n",
    "email: matt.e.garcia@gmail.com\n",
    "\n",
    "The base algorithm was extracted and developed from methods presented in my Ph.D. Dissertation:\n",
    "\n",
    "> Garcia, M., 2018: *Climatology and Forest Phenology during 1984-2013 around Western Lake Superior, USA*. Ph.D. Dissertation, College of Agriculture and Life Sciences, University of Wisconsin–Madison, 263 pp., ISBN 9780355631883, ProQuest document ID 2013767659.\n",
    "\n",
    "PI: Prof. Philip A. Townsend (ptownsend@wisc.edu)\n",
    "\n",
    "---\n",
    "\n",
    "**CANOPY-Phenology model structural development was continued in 2022-2024 by:**\n",
    "\n",
    "Matthew Garcia, Ph.D.  \n",
    "Research Scientist I  \n",
    "Dept. of Forest & Wildlife Ecology  \n",
    "University of Wisconsin - Madison  \n",
    "email: matt.e.garcia@gmail.com\n",
    "\n",
    "This code is made available under the GNU General Public License (GPL) v3.0. A copy of the GPLv3 is available from the Free Software Foundation at http://www.gnu.org/licenses/gpl.html.\n",
    "\n",
    "Users are asked to link to this GitHub repository in any derivative and/or published works. Users are expected to reference the attached publication in any derivative and/or published works. As of 31 January 2026, the preferred citation is:\n",
    "\n",
    "> Garcia, M., and P.A. Townsend, in prep.: \"CANOPY: Modeling seasonal and interannual temperate forest phenological variability using Landsat and weather observations.\" Manuscript in preparation for *MethodsX* with an accompanying paper in *Remote Sensing of Environment*.\n",
    "\n",
    "---\n",
    "\n",
    "**CANOPY is a backronym that stands for:**\n",
    "\n",
    "**C**onditioned  \n",
    "**A**nalysis of  \n",
    "**N**ormalized  \n",
    "**O**bservations in  \n",
    "**P**henological  \n",
    "**Y**ears  \n",
    "\n",
    "It comes from our nickname for the UW–Madison and USDA Forest Service project under which this phase of model development was completed.\n",
    "\n",
    "CANOPY Project co-PIs during 2022-2026 included:\n",
    "- Dr. Brian Sturtevant at the USDA Forest Service, Northern Research Station, Rhinelander, WI (brian.r.sturtevant@usda.gov)  \n",
    "- Dr. Therese Poland at the USDA Forest Service, Northern Research Station, Lansing, MI (therese.poland@usda.gov)  \n",
    "- Dr. Philip Townsend at the University of Wisconsin – Madison, Madison, WI (ptownsend@wisc.edu)  \n",
    "\n",
    "**This is the CANOPY-Phenology algorithm. Sometime soon we hope to have a follow-on project to build the CANOPY-Disturbance algorithm...**\n",
    "\n",
    "---\n",
    "\n",
    "**This CANOPY-Phenology model is oriented on phenological analysis and prediction on a single-pixel basis.**\n",
    "\n",
    "This notebook contains the basic functionality of the model:\n",
    "- fitting a custom-built long-term mean phenological curve against a single Landsat vegetation index (VI)\n",
    "- extraction of mean phenological curve indicators and metrics\n",
    "- PLS regression on climatological anomalies against the Landsat VI\n",
    "- PLS prediction of the Landsat VI in analyzed and forecast periods on an image basis\n",
    "- outlier detection using a forecast-interval approach\n",
    "- iterated outlier removal and re-modeling until an optimum model is obtained\n",
    "\n",
    "A separate notebook now handles the post-modeling procedure for daily VI prediction:\n",
    "- PLS prediction on a *daily* (instead of image) basis in user-specified analysis and forecast periods\n",
    "\n",
    "This notebook version is heavily modified from my original dissertation algorithm. Model version 0.99 already exceeds my dissertation model with the following features:\n",
    "- outlier detection (and PLS re-modeling) on the selected Landsat VI, rather than the KTTC components\n",
    "- automatic selection of PLS model components using minimum AICc (to prevent model overfitting)\n",
    "- calculation of variable importance in projection (VIP) in the PLS regression procedure\n",
    "- PLS variable reduction using the VIP metric (instead of a computationally intensive progressive elimination procedure)\n",
    "- image-based Landsat VI forecasting and evaluation metrics for a forecast (or other non-analysis) period\n",
    "- outlier detection using the predicted VI in the analysis period via prediction interval\n",
    "- iterated outlier removal and VI re-modeling until the optimum model (at minimum AICc) is obtained\n",
    "- daily Landsat VI prediction in the growing season on an annual basis for a user-specified year(s) in the analysis and/or forecast period\n",
    "- fitting the custom-built phenological curve against the predicted VI time series on an annual basis for the extraction of individual-year seasonal indicators and metrics\n",
    "\n",
    "This notebook is intended for testing and experiments on a single-pixel basis and is released with sample datasets as an open-source digital supplement to the above publication(s).\n",
    "\n",
    "Eventually, I intend to return some original (dissertation) functionality:\n",
    "- PLS regression and prediction on multiple Landsat VIs (other than KTTC components)\n",
    "- PLS regression and prediction on KTTC components (requires a different type of fitted phenological curve)\n",
    "- using climatological frost dates (mean and stdev) to specify VI curve fit SOS/EOS bounds\n",
    "\n",
    "Possible additional near-future version functionality may also include:\n",
    "- PLS regression and prediction on Landsat surface reflectance bands, instead of calculated VIs\n",
    "\n",
    "**This is a model intended for application to a *single Landsat pixel* using available observations.**\n",
    "\n",
    "**This procedure applies *no* spatial analysis at *any* scale to the model results.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 1**: load python libraries and set some notice and display parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook uses python standard libraries and common additional libraries\n",
    "#\n",
    "\n",
    "#\n",
    "# standard libraries, numpy, pandas, matplotlib, seaborn, scipy\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import csv\n",
    "from copy import deepcopy\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import linregress, kstest\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "#\n",
    "# PLS regression and modeling via scikit-learn library\n",
    "import sklearn\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#\n",
    "# suppress warnings about fragmented pandas DataFrames\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "#\n",
    "# suppress warnings about all the open figures\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "#\n",
    "# magic function to view graphs in notebook, instead of pop-up windows\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 2**: check current versions of python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python package versions')\n",
    "print(f'    python v. {platform.python_version()}')\n",
    "print(f'    numpy v. {np.__version__}')\n",
    "print(f'    pandas v. {pd.__version__}')\n",
    "print(f'    matplotlib v. {mpl.__version__}')\n",
    "print(f'    seaborn v. {sns.__version__}')\n",
    "print(f'    scipy v. {scipy.__version__}')\n",
    "print(f'    sklearn v. {sklearn.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 3**: establish the WxCD and VI variables available (based on established pre-processing methods, which can change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_vars.py\n",
    "#\n",
    "# NOTE as of v0.99 only the variable \"wxcd_std_anom_vars\" gets used outside of this cell\n",
    "#\n",
    "\n",
    "\n",
    "periods_days = [1, 3, 5, 10, 15, 30, 45, 60, 90, 120, 180, 270, 365]\n",
    "wxcd_base_vars = ['prcp', 'tavg', 'tmax', 'tmin', 'trange']\n",
    "wxcd_xdd_vars = ['tavg_cd', 'tavg_cdd', 'tavg_fd', 'tmin_fd',\n",
    "                 'tavg_gdd_base0', 'tavg_gdd_base5', 'tavg_gdd_base10']\n",
    "#\n",
    "Daymet_vars = []\n",
    "wxcd_std_anom_vars = []\n",
    "for v in wxcd_base_vars:\n",
    "    for p in periods_days:\n",
    "        if v == 'prcp':\n",
    "            Daymet_vars.append(f'{v}_{p}d_sum')\n",
    "        else:\n",
    "            Daymet_vars.append(f'{v}_{p}d_avg')\n",
    "            Daymet_vars.append(f'{v}_{p}d_std')\n",
    "        Daymet_vars.append(f'{v}_{p}d_clim_avg')\n",
    "        Daymet_vars.append(f'{v}_{p}d_clim_std')\n",
    "        Daymet_vars.append(f'{v}_{p}d_std_anom')\n",
    "        wxcd_std_anom_vars.append(f'{v}_{p}d_std_anom')\n",
    "#\n",
    "for v in wxcd_xdd_vars:\n",
    "    Daymet_vars.append(v)\n",
    "    Daymet_vars.append(f'{v}_clim_avg')\n",
    "    Daymet_vars.append(f'{v}_clim_std')\n",
    "    Daymet_vars.append(f'{v}_std_anom')\n",
    "    wxcd_std_anom_vars.append(f'{v}_std_anom')\n",
    "#\n",
    "Landsat_vars = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2',\n",
    "                'SR', 'MSI', 'NDVI', 'EVI', 'SAVI', 'NDMI', 'NBR',\n",
    "                'BGT', 'GRN', 'WET']\n",
    "kttc_vars = ['BGT', 'GRN', 'WET']\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_vars.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Input**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 4**: location ID, where to find its input data, and which VI to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required input to model_vi_phenology.py\n",
    "#\n",
    "# FUTURE WORK: convert to argparse usage for ALL metavals\n",
    "#\n",
    "\n",
    "\n",
    "point_id = '2023-120_CC'\n",
    "input_path = f'{point_id}_input'\n",
    "vi_name = 'EVI'\n",
    "\n",
    "\n",
    "#\n",
    "# end required input to model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 5**: configurable values used throughout the model procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of phenology_model_input.py\n",
    "#\n",
    "# metadata setup\n",
    "#\n",
    "# NOTE as of v0.99 there are switches for exploratory analyses and graph output (plots)\n",
    "#\n",
    "# FUTURE WORK: convert to argparse usage for ALL metavals\n",
    "#\n",
    "\n",
    "\n",
    "metavals = {'input_path': input_path,\n",
    "            'output_path': f'{point_id}_{vi_name.upper()}_analyses',\n",
    "            'point_id': point_id,\n",
    "            'daymet_input_fname': f'{point_id}_Daymet_data.csv',\n",
    "            'landsat_input_fname': f'{point_id}_Landsat_SR_VI.csv',\n",
    "            'n_observations': 0,\n",
    "            'x_basis': 'DOY',\n",
    "            'vi_name': vi_name.upper(),\n",
    "            'kttc_curve_type': 1,  # placeholder for return of KTTC processing\n",
    "            'kttc_curve_str': 'SIN',  # placeholder for return of KTTC processing\n",
    "            'vi_curve_type': 4,\n",
    "            'vi_curve_str': 'ABC',\n",
    "            'doy_min': 1,  # DOY\n",
    "            'doy_max': 365,  # DOY\n",
    "            'growing_season_start': 80,  # DOY of spring equinox\n",
    "            'growing_season_end': 355,  # DOY of winter solstice\n",
    "            'data_begin_year': 1984,\n",
    "            'data_end_year': 2023,\n",
    "            'analysis_begin_year': 1984,\n",
    "            'analysis_end_year': 2023,\n",
    "            'forecast_begin_year': 0,\n",
    "            'forecast_end_year': 0,\n",
    "            'nc_max': 20,\n",
    "            'test_split': 0.2,\n",
    "            'n_iterations': 500,\n",
    "            'vip_threshold': 1.0,\n",
    "            'remove_outliers': True,\n",
    "            'outlier_confidence': 0.95,\n",
    "            'outlier_round': 0,\n",
    "            'max_outlier_rounds': 3,\n",
    "            'n_outliers': 0,\n",
    "            'graph_output': True,\n",
    "            'exploratory': True}\n",
    "\n",
    "\n",
    "#\n",
    "# end part of phenology_model_input.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 6**: read the input dataset(s) for the location to be modeled, merge the VI and WxCD data, and do some initial clean-up and trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "# primary dataframe setup and initial reporting\n",
    "#\n",
    "\n",
    "\n",
    "print(f'Analyzing Landsat and WxCD data at {point_id}')\n",
    "#\n",
    "# if output path doesn't exist, make it\n",
    "output_path = metavals['output_path']\n",
    "if not os.path.exists(output_path):\n",
    "    print(f'- making output path {output_path}')\n",
    "    os.makedirs(output_path)\n",
    "#\n",
    "# read Landsat input data\n",
    "input_path = metavals['input_path']\n",
    "landsat_input_fname = metavals['landsat_input_fname']\n",
    "landsat_input_fpath = f'{input_path}/{landsat_input_fname}'\n",
    "print(f'- reading Landsat input file {landsat_input_fname}')\n",
    "vi_input_df = pd.read_csv(landsat_input_fpath, index_col=None)\n",
    "#\n",
    "# read Daymet input data\n",
    "daymet_input_fname = metavals['daymet_input_fname']\n",
    "daymet_input_fpath = f'{input_path}/{daymet_input_fname}'\n",
    "print(f'- reading Daymet input file {daymet_input_fname}')\n",
    "wxcd_input_df = pd.read_csv(daymet_input_fpath, index_col=None)\n",
    "print()\n",
    "#\n",
    "# merge input DataFrames\n",
    "vi_input_df.drop(columns=['QA_condition'], inplace=True)  # all input dates are 'clear'\n",
    "year = vi_input_df['Year']\n",
    "month = vi_input_df['Month']\n",
    "day = vi_input_df['Day']\n",
    "vi_input_df['Date'] = [f'{yyyy}{mm:02d}{dd:02d}' for yyyy, mm, dd in zip(year, month, day)]\n",
    "doy = vi_input_df['DOY']\n",
    "vi_input_df['DYear'] = [float(yyyy + (d/366)) for yyyy, d in zip(year, doy)]\n",
    "wxcd_input_df.drop(columns=['point_id', 'Year', 'DOY'], inplace=True)\n",
    "wxcd_input_df.fillna(0, inplace=True)\n",
    "wxcd_vi_input_df = pd.merge(vi_input_df, wxcd_input_df, on='YYYY_DOY')\n",
    "n_input_data_rows = len(wxcd_vi_input_df)\n",
    "data_begin_year = metavals['data_begin_year']\n",
    "data_end_year = metavals['data_end_year']\n",
    "print(f'Data for {point_id} span {data_begin_year}-{data_end_year} with {n_input_data_rows} rows')\n",
    "#\n",
    "# clean DataFrame of rows with missing data\n",
    "wxcd_vi_all_df = wxcd_vi_input_df.dropna()\n",
    "n_data_rows = len(wxcd_vi_all_df)\n",
    "if n_data_rows < n_input_data_rows:\n",
    "    print(f'- removing missing data leaves {n_data_rows} rows')\n",
    "#\n",
    "# trim DataFrame to analysis period (if different from data coverage period)\n",
    "if (metavals['data_begin_year'] == metavals['analysis_begin_year']) and \\\n",
    "        (metavals['data_end_year'] == metavals['analysis_end_year']):\n",
    "    print(f'- using all available years for analysis ({n_data_rows} rows)')\n",
    "    wxcd_vi_model_df = deepcopy(wxcd_vi_all_df)\n",
    "else:\n",
    "    wxcd_vi_model_df = wxcd_vi_all_df[wxcd_vi_all_df['Year'] >= metavals['analysis_begin_year']]\n",
    "    wxcd_vi_model_df = wxcd_vi_model_df[wxcd_vi_model_df['Year'] <= metavals['analysis_end_year']]\n",
    "    print(f'- trimming data to specified analysis years leaves {len(wxcd_vi_model_df)} rows')\n",
    "#\n",
    "# trim DataFrame to growing season\n",
    "wxcd_vi_df = wxcd_vi_model_df[wxcd_vi_model_df['DOY'] >= metavals['growing_season_start']]\n",
    "wxcd_vi_df = wxcd_vi_df[wxcd_vi_df['DOY'] <= metavals['growing_season_end']]\n",
    "print(f'- trimming to specified growing season DOY range leaves {len(wxcd_vi_df)} rows')\n",
    "#\n",
    "# get n_observations\n",
    "metavals['n_observations'] = len(wxcd_vi_df)\n",
    "#\n",
    "# save input DataFrame\n",
    "outfname = f'{point_id}_input_WxCD_VI_cleaned_trimmed.csv'\n",
    "outfpath = f'{output_path}/{outfname}'\n",
    "wxcd_vi_df.to_csv(outfpath, index=False)\n",
    "print(f'- saved {outfname}')\n",
    "print()\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Standard Seasonal Phenology Model and Metrics**\n",
    "\n",
    "<p>The phenological curve fitted to the user's selected Landsat VI is described by a 7-parameter piecewise-continuous function using two halves of a cosine curve (for the green-up and senescence portions) and a linear function with green-down effect (for the mature period). There is some additional math to help smooth the curve at the junctions. The ordinary least-squares (OLS) fit is based on the entire drawn curve, not on individual pieces of the curve. The resulting curve was illustrated in figure 4.2 (p. 58) of my dissertation, based on fitting an actual single-pixel Landsat time series:</p>\n",
    "\n",
    "<img src=\"./phenology_model_curve_indicators_schematic.png\" alt=\"Phenological curve indicators\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "<p>All of the indicators and metrics for the fitted curve are calculated and reported below. This curve is approximately valid for almost all forest types, even evergreens, and the formulation allows a mature period that is flat or even reverse-slope (maximum VI just before senescence, instead of just after green-up) if that is found to fit better. I am working to re-implement two constraints on the curve, specifically on the earliest SOS and latest EOS, using climatological spring and autumn frost dates, respectively. That requires further analysis than I have completed thus far in my climatological reprocessing work. For now, the open values for SOSmin and EOSmax provided below seem to work well.</p>\n",
    "\n",
    "<p>NOTE that this curve is *not* appropriate for some VIs, such as the Tasseled Cap Greenness (GRN) and Wetness (WET). I plan to restore a fitted sine curve function that I've used before for those as part of this process, and the user will choose which curve type gets fitted to their desired VI.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 7**: calculate numerous error metrics for a fitted phenological curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_error_metrics.py\n",
    "#\n",
    "\n",
    "\n",
    "def calc_fit_error_metrics(n_obs, n_params, values, residuals):\n",
    "    bias = np.mean(residuals)\n",
    "    mae = np.mean(np.abs(residuals))\n",
    "    rmse = np.sqrt(np.mean(residuals**2))\n",
    "    sse = np.sum(residuals**2)\n",
    "    values_mean = np.mean(values)\n",
    "    sst = np.sum((values - values_mean)**2)\n",
    "    rsq = 1 - (sse / sst)\n",
    "    #\n",
    "    residuals_mean = np.mean(residuals)\n",
    "    residuals_var = np.var(residuals)\n",
    "    ssd = np.sum((residuals - residuals_mean)**2)\n",
    "    #\n",
    "    # formula for log-L from Wikipedia:\n",
    "    #     https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n",
    "    # assuming residuals have a normal distribution\n",
    "    #\n",
    "    log_L = -1 * (n_obs / 2) * np.log(2 * np.pi * residuals_var) - (ssd / (2 * residuals_var))\n",
    "    aic = 2.0 * (n_params - log_L)\n",
    "    if (n_obs - n_params) > 2:\n",
    "        aicc = aic + (2 * (n_params + 1) * (n_params + 2) / (n_obs - n_params - 2))\n",
    "    else:\n",
    "        aicc = aic\n",
    "    bic = (n_params * np.log(n_obs)) - (2.0 * log_L)\n",
    "    #\n",
    "    fit_error_metrics = [bias, mae, rmse, sse, rsq, aic, aicc, bic]\n",
    "    #\n",
    "    return fit_error_metrics\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_error_metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 8**: definition and implementation of the ABC curve fit procedure\n",
    "\n",
    "Calls function in **Notebook Cell 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_fit_abc_curve.py\n",
    "#\n",
    "\n",
    "\n",
    "#\n",
    "# parameters of the ABC curve\n",
    "abc_curve_fit_vars = ['Ymin', 'Ymin_err',\n",
    "                      'SOS', 'SOS_err',\n",
    "                      'Ymax_a', 'Ymax_a_err',\n",
    "                      'Ymax_a_doy', 'Ymax_a_doy_err',\n",
    "                      'Ymax_b', 'Ymax_b_err',\n",
    "                      'Ymax_b_doy', 'Ymax_b_doy_err',\n",
    "                      'EOS', 'EOS_err',\n",
    "                      'Bias', 'MAE', 'RMSE', 'SSE', 'Rsq',\n",
    "                      'AIC', 'AICc', 'BIC']\n",
    "\n",
    "\n",
    "#\n",
    "# mathematical definition of the ABC curve\n",
    "def abc_fn(X_sample, Ymin, SOS, Ymax_a, Ymax_a_X, Ymax_b, Ymax_b_X, EOS):\n",
    "    \"\"\"\n",
    "    builds a piecewise continuous asymmetric broken cosine phenological curve\n",
    "    1. fit a cosine curve to the data (x = 0 at mid-summer)\n",
    "    2. break it at the peak and separate the halves\n",
    "    3. allow the halves to have different parameters (period & amplitude)\n",
    "    \"\"\"\n",
    "    Xmin = int(X_sample[-2])\n",
    "    Xmax = int(X_sample[-1])\n",
    "    X_sample = np.array(X_sample[:-2]).astype(int)\n",
    "    X_offset = 10\n",
    "    X = np.arange(Xmin, Xmax + 1)\n",
    "    Y = np.zeros(np.shape(X))\n",
    "    #\n",
    "    # in this loop, draw most of the phenological curve\n",
    "    # - Y = Ymin for pre- and post-season periods\n",
    "    # - Y = cosine curves for green-up and senescence periods\n",
    "    # - leave mature period for quasi-adaptive method below\n",
    "    #\n",
    "    for yt, xt in enumerate(X):\n",
    "        if xt < SOS:                                                          # pre-season phenophase\n",
    "            Y[yt] = Ymin\n",
    "        elif (xt >= SOS) and (xt < (Ymax_a_X + X_offset)):                    # green-up phenophase\n",
    "            amp = (Ymax_a - Ymin) / 2.0\n",
    "            xt_rad = ((xt - Ymax_a_X) / float(Ymax_a_X - SOS)) * np.pi\n",
    "            Y[yt] = Ymin + amp * (1 + np.cos(xt_rad))\n",
    "        elif (xt >= (Ymax_a_X + X_offset)) and (xt < (Ymax_b_X - X_offset)):  # mature phenophase\n",
    "            pass  # see below\n",
    "        elif (xt >= (Ymax_b_X - X_offset)) and (xt < EOS):                    # senescence phenophase\n",
    "            amp = (Ymax_b - Ymin) / 2.0\n",
    "            xt_rad = ((xt - Ymax_b_X) / float(EOS - Ymax_b_X)) * np.pi\n",
    "            Y[yt] = Ymin + amp * (1 + np.cos(xt_rad))\n",
    "        elif xt >= EOS:                                                       # post-season phenophase\n",
    "            Y[yt] = Ymin\n",
    "    #\n",
    "    # quasi-adaptive method for smoother curves around the mature transitions\n",
    "    # - matches mature phenophase slope with the green-up and senescence limbs\n",
    "    #\n",
    "    slope = (Ymax_b - Ymax_a) / float(Ymax_b_X - Ymax_a_X)\n",
    "    if slope > 0:\n",
    "        for xt_a in range(int(round(Ymax_a_X) - X_offset),\n",
    "                          int(round(Ymax_a_X) - 1)):\n",
    "            slope_a = Y[xt_a + 1] - Y[xt_a]\n",
    "            if slope_a <= slope:\n",
    "                break\n",
    "        for xt_b in range(int(round(Ymax_b_X) - X_offset + 1),\n",
    "                          int(round(Ymax_b_X))):\n",
    "            slope_b = Y[xt_b] - Y[xt_b - 1]\n",
    "            if slope_b <= slope:\n",
    "                break\n",
    "        slope = (Y[xt_b] - Y[xt_a]) / float(xt_b - xt_a)\n",
    "        for yt in range(xt_a, xt_b):\n",
    "            Y[yt] = Y[xt_a] + slope * (yt - xt_a)\n",
    "    elif slope < 0:\n",
    "        for xt_a in range(int(round(Ymax_a_X)),\n",
    "                          int(round(Ymax_a_X) + X_offset - 1)):\n",
    "            slope_a = Y[xt_a + 1] - Y[xt_a]\n",
    "            if slope_a <= slope:\n",
    "                break\n",
    "        for xt_b in range(int(round(Ymax_b_X)),\n",
    "                          int(round(Ymax_b_X) + X_offset - 1)):\n",
    "            slope_b = Y[xt_b + 1] - Y[xt_b]\n",
    "            if slope_b <= slope:\n",
    "                break\n",
    "        slope = (Y[xt_b] - Y[xt_a]) / float(xt_b - xt_a)\n",
    "        for yt in range(xt_a, xt_b):\n",
    "            Y[yt] = Y[xt_a] + slope * (yt - xt_a)\n",
    "    else:\n",
    "        for yt in range(int(round(Ymax_a_X)), int(round(Ymax_b_X))):\n",
    "            Y[yt] = Ymax_a\n",
    "    #\n",
    "    # return curve values at only the sample dates\n",
    "    Y_fit = np.zeros(len(X_sample))\n",
    "    for d, doy in enumerate(X_sample):\n",
    "        Y_fit[d] = Y[doy - 1]\n",
    "    #\n",
    "    return Y_fit\n",
    "\n",
    "\n",
    "#\n",
    "# construct an ABC curve from known parameter values\n",
    "def get_abc_curve(x_range, params):\n",
    "    fit_y = abc_fn(x_range, params[0], params[2], params[4],\n",
    "                   params[6], params[8], params[10], params[12])\n",
    "    #\n",
    "    return fit_y\n",
    "\n",
    "\n",
    "#\n",
    "# fit an ABC curve to the given data and report parameter values and error metrics\n",
    "def optimize_fit_abc(X, Y, X_min, X_max):\n",
    "    #\n",
    "    # initial parameter values\n",
    "    p_init = [0.0, 90, 1.0, 120, 1.0, 240, 270]\n",
    "    #\n",
    "    # parameter bounds for fit process\n",
    "    lower_bounds = [-1.0, 1, -1.0, 1, -1.0, 228, 228]\n",
    "    upper_bounds = [1.0, 227, 1.0, 227, 1.0, 365, 365]\n",
    "    #\n",
    "    # sneak the overall bounds of X into the curve_fit argument structure\n",
    "    X_ext = np.append(X, [X_min, X_max])\n",
    "    #\n",
    "    # bounded ordinary least-squares (OLS) curve fit process via scipy.optimize.curve_fit\n",
    "    p_opt, p_cov = curve_fit(abc_fn, X_ext, Y, p0=p_init,\n",
    "                             bounds=(lower_bounds, upper_bounds),\n",
    "                             method='trf', max_nfev=1E6)\n",
    "    #\n",
    "    # fitted curve error metrics\n",
    "    p_err = np.sqrt(np.diag(p_cov))\n",
    "    pY = abc_fn(X_ext, p_opt[0], p_opt[1], p_opt[2],\n",
    "                p_opt[3], p_opt[4], p_opt[5], p_opt[6])\n",
    "    Y_res = pY - Y\n",
    "    #\n",
    "    fit_error_metrics = calc_fit_error_metrics(len(X), len(p_opt), Y, Y_res)\n",
    "    #\n",
    "    return p_opt, p_err, fit_error_metrics\n",
    "\n",
    "\n",
    "#\n",
    "# entry to the curve fit procedure\n",
    "def pheno_fit_abc(xvals, yvals, xmin=1, xmax=365):\n",
    "    fit, fit_err, fit_error_metrics = optimize_fit_abc(xvals, yvals, xmin, xmax)\n",
    "    pheno_params = [fit[0], fit_err[0], fit[1], fit_err[1],\n",
    "                    fit[2], fit_err[2], fit[3], fit_err[3],\n",
    "                    fit[4], fit_err[4], fit[5], fit_err[5],\n",
    "                    fit[6], fit_err[6]]\n",
    "    pheno_params += fit_error_metrics\n",
    "    x_range = list(np.arange(xmin, xmax + 1))\n",
    "    x_range_ext = x_range + [xmin, xmax]\n",
    "    fit_all_y = abc_fn(x_range_ext, fit[0], fit[1], fit[2],\n",
    "                       fit[3], fit[4], fit[5], fit[6])\n",
    "    fit_sample_y = np.array([fit_all_y[int(val) - 1] for val in xvals])\n",
    "    pheno_anom = yvals - fit_sample_y\n",
    "    #\n",
    "    return pheno_params, pheno_anom\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_fit_abc_curve.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 9**: plot VI values by decimal year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not yet part of phenology_model_plots.py\n",
    "#\n",
    "\n",
    "\n",
    "def plot_vi_by_dyear(vi_name, vi_dyears, vi_values, begin_year, end_year,\n",
    "                     outlier_round, output_path, fname_tuple):\n",
    "    #\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    #\n",
    "    # plot VI values by DYear\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    plt.scatter(vi_dyears, vi_values, marker='x', c='k', s=30)\n",
    "    plt.xlim([begin_year, end_year+1])\n",
    "    years = np.arange(begin_year, end_year+2)\n",
    "    even_years = [year if year % 2 == 0 else '' for year in years]\n",
    "    plt.xticks(years, fontsize=10)\n",
    "    ax.set_xticklabels(even_years, fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('Year', fontsize=11)\n",
    "    plt.ylabel(vi_name, fontsize=11)\n",
    "    title = f'{begin_year}-{end_year} {vi_name} observations '\n",
    "    title += f'(round {outlier_round}, n = {len(vi_values)})'\n",
    "    plt.title(title, fontsize=12)\n",
    "    #\n",
    "    plt.tight_layout()\n",
    "    plotfname = '%s_%s_%s_%s_round_%d_obs_by_dyear.png' % fname_tuple\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    #\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 10**: plot VI values by observation day-of-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived from part of phenology_model_plots.py\n",
    "#\n",
    "\n",
    "\n",
    "def plot_vi_by_doy(vi_name, vi_doys, vi_values, doys_range,\n",
    "                   season_start, season_end, begin_year, end_year,\n",
    "                   outlier_round, output_path, fname_tuple, vi_outliers_df):\n",
    "    #\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    #\n",
    "    # plot VI values by DOY\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    plt.scatter(vi_doys, vi_values, marker='x', c='k', s=30)\n",
    "    if len(vi_outliers_df):\n",
    "        outliers_doys = np.array(vi_outliers_df['DOY'])\n",
    "        outliers_values = np.array(vi_outliers_df[vi_name])\n",
    "        plt.scatter(outliers_doys, outliers_values, marker='o', c='r',\n",
    "                    edgecolor='k', s=30)\n",
    "    plt.xlim([season_start, season_end])\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('DOY', fontsize=11)\n",
    "    plt.ylabel(vi_name, fontsize=11)\n",
    "    title = f'{begin_year}-{end_year} {vi_name} observations '\n",
    "    title += f'(round {outlier_round}, n = {len(vi_values)})'\n",
    "    plt.title(title, fontsize=12)\n",
    "    #\n",
    "    plt.tight_layout()\n",
    "    plotfname = '%s_%s_%s_%s_round_%d_obs_by_doy.png' % fname_tuple\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    #\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 11**: plot fitted phenological curve along with VI values by day-of-year, with outliers (if any) marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived from part of phenology_model_plots.py\n",
    "#\n",
    "\n",
    "\n",
    "def plot_vi_curve_fit(vi_name, vi_doys, vi_values, full_year_mean_vi, \n",
    "                      rsq, doys_range, season_start, season_end,\n",
    "                      begin_year, end_year, var_fit_params, outlier_round,\n",
    "                      output_path, fname_tuple, vi_outliers_df):\n",
    "    #\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    #\n",
    "    # plot VI values by DOY\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    plt.scatter(vi_doys, vi_values, marker='x', c='k', s=30)\n",
    "    if len(vi_outliers_df):\n",
    "        outliers_doys = np.array(vi_outliers_df['DOY'])\n",
    "        outliers_values = np.array(vi_outliers_df[vi_name])\n",
    "        plt.scatter(outliers_doys, outliers_values, marker='o', c='r',\n",
    "                    edgecolor='k', s=30)\n",
    "    plt.plot(doys_range, full_year_mean_vi, 'b-', linewidth=2)\n",
    "    control_doys = [var_fit_params[2], var_fit_params[6],\n",
    "                    var_fit_params[10], var_fit_params[12]]\n",
    "    control_vi_values = [full_year_mean_vi[int(round(var_fit_params[2])) - 1],\n",
    "                         var_fit_params[4], var_fit_params[8], \n",
    "                         full_year_mean_vi[int(round(var_fit_params[12])) - 1]]\n",
    "    plt.scatter(control_doys, control_vi_values, marker='o', c='g', edgecolor='g', s=50)\n",
    "    plt.xlim([season_start, season_end])\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('DOY', fontsize=11)\n",
    "    plt.ylabel(vi_name, fontsize=11)\n",
    "    title = f'{begin_year}-{end_year} {vi_name} mean phenocurve '\n",
    "    title += f'(round {outlier_round}, Rsq = {rsq:.3f})'\n",
    "    plt.title(title, fontsize=12)\n",
    "    #\n",
    "    plt.tight_layout()\n",
    "    plotfname = '%s_%s_%s_%s_round_%d_obs+phenocurve.png' % fname_tuple\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    #\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 12**: subtract fitted phenological curve from VI values by day-of-year and plot residuals, with outliers (if any) marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived from part of phenology_model_plots.py\n",
    "#\n",
    "\n",
    "\n",
    "def plot_vi_residuals(vi_name, vi_doys, vi_fit_residuals, full_year_mean_vi, \n",
    "                      doys_range, season_start, season_end,\n",
    "                      begin_year, end_year, outlier_round,\n",
    "                      output_path, fname_tuple, vi_outliers_df):\n",
    "    #\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    #\n",
    "    # plot VI values by DOY\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    plt.scatter(vi_doys, vi_fit_residuals, marker='x', c='k', s=30)\n",
    "    if len(vi_outliers_df):\n",
    "        outliers_doys = np.array(vi_outliers_df['DOY'])\n",
    "        outliers_values = np.array(vi_outliers_df[vi_name])\n",
    "        curve_values = np.array([full_year_mean_vi[int(doy) - 1]\n",
    "                                 for doy in outliers_doys])\n",
    "        outliers_residuals = outliers_values - curve_values\n",
    "        plt.scatter(outliers_doys, outliers_residuals, marker='o',\n",
    "                    c='r', edgecolor='k', s=30)\n",
    "    plt.plot(doys_range, np.zeros_like(full_year_mean_vi), 'b-', linewidth=2)\n",
    "    plt.xlim([season_start, season_end])\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('DOY', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    title = f'{begin_year}-{end_year} {vi_name} residuals (round {outlier_round})'\n",
    "    plt.title(title, fontsize=12)\n",
    "    #\n",
    "    plt.tight_layout()\n",
    "    plotfname = '%s_%s_%s_%s_round_%d_residuals.png' % fname_tuple\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    #\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 13**: plot a histogram of the VI residuals from **Notebook Cell 12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived from part of phenology_model_plots.py\n",
    "#\n",
    "\n",
    "\n",
    "def plot_vi_residuals_histogram(vi_name, vi_fit_residuals, begin_year, end_year,\n",
    "                                ks_value, ks_pvalue, outlier_round, output_path,\n",
    "                                fname_tuple, vi_outliers_df):\n",
    "    #\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    #\n",
    "    # plot VI values by DOY\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    nbins = int(round((np.max(vi_fit_residuals) - np.min(vi_fit_residuals)) / 0.01))\n",
    "    plt.hist(vi_fit_residuals, bins=nbins)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(f'{vi_name} residuals', fontsize=11)\n",
    "    plt.ylabel('count', fontsize=11)\n",
    "    title = f'{begin_year}-{end_year} {vi_name} residuals '\n",
    "    title += f'(round {outlier_round}, K-S test = {ks_value:.3f}, p = {ks_pvalue:.3f})'\n",
    "    plt.title(title, fontsize=12)\n",
    "    #\n",
    "    plt.tight_layout()\n",
    "    plotfname = '%s_%s_%s_%s_round_%d_residuals_histogram.png' % fname_tuple\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    #\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Procedure for Fitting the Mean Phenological Curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 14**: Fit the long-term mean phenological curve to the selected VI observations and plot/save the results.\n",
    "\n",
    "Calls functions in **Notebook Cells 8, 9, 10, 11, and 12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_vi.py\n",
    "#\n",
    "\n",
    "\n",
    "def model_vi_pheno_curve(metavals, vi_retained_df, vi_outliers_df=pd.DataFrame()):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    x_basis = metavals['x_basis']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    curve_fit_vars = abc_curve_fit_vars\n",
    "    begin_year = metavals['analysis_begin_year']\n",
    "    end_year = metavals['analysis_end_year']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    remove_outliers = metavals['remove_outliers']\n",
    "    n_outliers = metavals['n_outliers']\n",
    "    doy_min = metavals['doy_min']\n",
    "    doy_max = metavals['doy_max']\n",
    "    growing_season_start = metavals['growing_season_start']\n",
    "    growing_season_end = metavals['growing_season_end']\n",
    "    graph_output = metavals['graph_output']\n",
    "    fname_tuple = (point_id, vi_name, curve_str, x_basis, outlier_round)\n",
    "    #\n",
    "    #\n",
    "    print(f'Round {outlier_round}: {n_outliers} outliers identified')\n",
    "    print(f'Step 0: fit {curve_str} phenology curve to {vi_name}')\n",
    "    print()\n",
    "    #\n",
    "    # initialize table for VI curve fit parameters\n",
    "    pheno_fit_params_df = pd.DataFrame({'parameter': curve_fit_vars})\n",
    "    #\n",
    "    # initialize table for fitted curve VI anomalies\n",
    "    # note that only the \"retained\" data are used, with no outliers present\n",
    "    pheno_fit_residuals_df = pd.DataFrame({'Date': np.array(vi_retained_df['Date']),\n",
    "                                           'DYear': np.array(vi_retained_df['DYear']),\n",
    "                                           'DOY': np.array(vi_retained_df['DOY'])})\n",
    "    #\n",
    "    # get DYear, DOY, and VI values\n",
    "    vi_dyears = np.array(vi_retained_df['DYear'])\n",
    "    vi_doys = np.array(vi_retained_df[x_basis]).astype(int)\n",
    "    vi_values = np.array(vi_retained_df[vi_name])\n",
    "    #\n",
    "    # plot VI observations by DYear\n",
    "    if graph_output:\n",
    "        plot_vi_by_dyear(vi_name, vi_dyears, vi_values, begin_year, end_year,\n",
    "                         outlier_round, output_path, fname_tuple)\n",
    "    print()\n",
    "    #\n",
    "    # fit VI mean phenology curve\n",
    "    print(f'Fitting {vi_name} by {x_basis}')\n",
    "    vi_fit_params, vi_fit_residuals = pheno_fit_abc(vi_doys, vi_values, doy_min, doy_max)\n",
    "    vi_doys_ext = np.append(vi_doys, [doy_min, doy_max])\n",
    "    vi_fitted = get_abc_curve(vi_doys_ext, vi_fit_params)\n",
    "    pheno_fit_params_df[f'{vi_name}_by_{x_basis}'] = vi_fit_params\n",
    "    pheno_fit_residuals_df[vi_name] = vi_values\n",
    "    pheno_fit_residuals_df[f'{vi_name}_fitted'] = vi_fitted\n",
    "    pheno_fit_residuals_df[f'{vi_name}_residual'] = vi_fit_residuals\n",
    "    rsq = vi_fit_params[curve_fit_vars.index('Rsq')]\n",
    "    print(f'- Rsq = {rsq:.3f}')\n",
    "    print()\n",
    "    #\n",
    "    # get full-year daily VI values from fit parameters\n",
    "    doys_range = np.arange(doy_min, doy_max + 1)\n",
    "    doys_range_ext = np.append(doys_range, [doy_min, doy_max])\n",
    "    full_year_mean_vi = get_abc_curve(doys_range_ext, vi_fit_params)\n",
    "    #\n",
    "    # plot VI obervations by DOY with marked outliers\n",
    "    if graph_output:\n",
    "        plot_vi_by_doy(vi_name, vi_doys, vi_values, doys_range,\n",
    "                       growing_season_start, growing_season_end, begin_year, end_year,\n",
    "                       outlier_round, output_path, fname_tuple, vi_outliers_df)\n",
    "    #\n",
    "    # plot VI obervations by DOY with marked outliers and fitted curve, and VI residuals\n",
    "    if graph_output:\n",
    "        plot_vi_curve_fit(vi_name, vi_doys, vi_values, full_year_mean_vi, \n",
    "                          rsq, doys_range, growing_season_start, growing_season_end,\n",
    "                          begin_year, end_year, vi_fit_params, outlier_round,\n",
    "                          output_path, fname_tuple, vi_outliers_df)\n",
    "    #\n",
    "    # plot VI residuals by DOY with marked outliers\n",
    "    if graph_output:\n",
    "        plot_vi_residuals(vi_name, vi_doys, vi_fit_residuals, full_year_mean_vi, \n",
    "                          doys_range, growing_season_start, growing_season_end,\n",
    "                          begin_year, end_year, outlier_round,\n",
    "                          output_path, fname_tuple, vi_outliers_df)\n",
    "    #\n",
    "    # Check VI residuals for normal distribution using scipy.stats.kstest\n",
    "    ks_statistic, ks_pvalue = kstest(vi_fit_residuals, 'norm')\n",
    "    print('Kolmogorov-Smirnov test for normality of residuals')\n",
    "    print(f'- statistic = {ks_statistic:.3f}')\n",
    "    print(f'- p = {ks_pvalue:.3f}')\n",
    "    print()\n",
    "    #\n",
    "    # plot VI residuals histogram\n",
    "    if graph_output:\n",
    "        plot_vi_residuals_histogram(vi_name, vi_fit_residuals, begin_year, end_year,\n",
    "                                    ks_statistic, ks_pvalue, outlier_round, output_path,\n",
    "                                    fname_tuple, vi_outliers_df)\n",
    "    print()\n",
    "    #\n",
    "    # save VI curve fit parameters to CSV\n",
    "    outfname = '%s_retained_%s_%s_%s_round_%d_fit_params.csv' % fname_tuple\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    pheno_fit_params_df.to_csv(outfpath, index=False)\n",
    "    print(f'saved {outfname}')\n",
    "    #\n",
    "    # save retained VI data with curve-based anomalies to CSV\n",
    "    outfname = '%s_retained_%s_%s_%s_round_%d_fit_residuals.csv' % fname_tuple\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    pheno_fit_residuals_df.to_csv(outfpath, index=False)\n",
    "    print(f'saved {outfname}')\n",
    "    #\n",
    "    # save full-year mean VI curve to NPY\n",
    "    outfname = '%s_retained_%s_%s_%s_round_%d_full_year.npy' % fname_tuple\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    np.save(outfpath, full_year_mean_vi)\n",
    "    print(f'saved {outfname}')\n",
    "    print()\n",
    "    #\n",
    "    return pheno_fit_params_df, pheno_fit_residuals_df, full_year_mean_vi\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_vi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 15**: invoke function to fit the long-term mean phenological curve\n",
    "\n",
    "Calls function in **Notebook Cell 14**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "\n",
    "\n",
    "pheno_fit_params_df, pheno_fit_residuals_df, full_year_mean_vi = \\\n",
    "    model_vi_pheno_curve(metavals, wxcd_vi_df)\n",
    "pheno_fit_residuals_orig_df = deepcopy(pheno_fit_residuals_df)\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 16**: statistics to support phenological metric calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of phenology_model_stats.py\n",
    "#\n",
    "\n",
    "\n",
    "def additive_error(var1_err, var2_err):\n",
    "    err = np.sqrt(var1_err**2 + var2_err**2)\n",
    "    #\n",
    "    return err\n",
    "\n",
    "\n",
    "def multiplicative_error(var0, var1, var1_err, var2, var2_err):\n",
    "    err = np.sqrt(var0**2 * ((var1_err / var1)**2 + (var2_err / var2)**2))\n",
    "    #\n",
    "    return err\n",
    "\n",
    "\n",
    "def regress(x, y):\n",
    "    # uses scipy.stats.linregress\n",
    "    slope, intercept, corr, sig, stderr = linregress(x, y)\n",
    "    #\n",
    "    return [slope, intercept, corr, sig, stderr]\n",
    "\n",
    "\n",
    "#\n",
    "# end part of phenology_model_stats.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 17**: phenological metric calculations\n",
    "\n",
    "Calls functions in **Notebook Cell 16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of phenology_model_vi_metrics.py\n",
    "#\n",
    "\n",
    "\n",
    "def calc_fitted_pheno_curve_metrics(vi_name, curve_params):\n",
    "    \"\"\"Calculate indicators and metrics for fitted phenological curve.\"\"\"\n",
    "    #\n",
    "    labels = []\n",
    "    values = []\n",
    "    errors = []\n",
    "    #\n",
    "    # clearly identify fitted curve indicators\n",
    "    labels.append(f'min {vi_name}')\n",
    "    VI_min = curve_params[0]\n",
    "    values.append(VI_min)\n",
    "    VI_min_err = curve_params[1]\n",
    "    errors.append(VI_min_err)\n",
    "    #\n",
    "    labels.append(f'{vi_name} SOS [DOY]')\n",
    "    SOS = curve_params[2]\n",
    "    values.append(SOS)\n",
    "    SOS_err = curve_params[3]\n",
    "    errors.append(SOS_err)\n",
    "    #\n",
    "    labels.append(f'Spring max {vi_name}')\n",
    "    SVI_max = curve_params[4]\n",
    "    values.append(SVI_max)\n",
    "    SVI_max_err = curve_params[5]\n",
    "    errors.append(SVI_max_err)\n",
    "    #\n",
    "    labels.append(f'{vi_name} SOM [DOY]')\n",
    "    SOM = curve_params[6]\n",
    "    values.append(SOM)\n",
    "    SOM_err = curve_params[7]\n",
    "    errors.append(SOM_err)\n",
    "    #\n",
    "    labels.append(f'Autumn max {vi_name}')\n",
    "    AVI_max = curve_params[8]\n",
    "    values.append(AVI_max)\n",
    "    AVI_max_err = curve_params[9]\n",
    "    errors.append(AVI_max_err)\n",
    "    #\n",
    "    labels.append(f'{vi_name} EOM [DOY]')\n",
    "    EOM = curve_params[10]\n",
    "    values.append(EOM)\n",
    "    EOM_err = curve_params[11]\n",
    "    errors.append(EOM_err)\n",
    "    #\n",
    "    labels.append(f'{vi_name} EOA [DOY]')\n",
    "    EOA = curve_params[12]\n",
    "    values.append(EOA)\n",
    "    EOA_err = curve_params[13]\n",
    "    errors.append(EOA_err)\n",
    "    #\n",
    "    # calculate fitted-curve-based metrics\n",
    "    labels.append(f'Spring {vi_name} range')\n",
    "    SVI_range = SVI_max - VI_min\n",
    "    values.append(SVI_range)\n",
    "    SVI_range_err = additive_error(VI_min_err, SVI_max_err)\n",
    "    errors.append(SVI_range_err)\n",
    "    #\n",
    "    labels.append(f'Spring {vi_name} inflection')\n",
    "    VI_SI = (VI_min + SVI_max) / 2.0\n",
    "    values.append(VI_SI)\n",
    "    VI_SI_err = SVI_range_err\n",
    "    errors.append(VI_SI_err)\n",
    "    #\n",
    "    labels.append(f'Spring {vi_name} inflection [DOY]')\n",
    "    SI = (SOS + SOM) / 2.0\n",
    "    values.append(SI)\n",
    "    SI_err = additive_error(SOS_err, SOM_err)\n",
    "    errors.append(SI_err)\n",
    "    #\n",
    "    labels.append(f'Spring {vi_name} duration [d]')\n",
    "    DOS = SOM - SOS\n",
    "    values.append(DOS)\n",
    "    DOS_err = SI_err\n",
    "    errors.append(DOS_err)\n",
    "    #\n",
    "    labels.append(f'Spring {vi_name} slope [1/d]')\n",
    "    Slope_of_Spring = SVI_range / DOS\n",
    "    values.append(Slope_of_Spring)\n",
    "    Slope_of_Spring_err = multiplicative_error(Slope_of_Spring, SVI_range, SVI_range_err,\n",
    "                                               DOS, DOS_err)\n",
    "    errors.append(Slope_of_Spring_err)\n",
    "    #\n",
    "    labels.append(f'Maturity {vi_name} duration [d]')\n",
    "    DOM = EOM - SOM\n",
    "    values.append(DOM)\n",
    "    DOM_err = additive_error(SOM_err, EOM_err)\n",
    "    errors.append(DOM_err)\n",
    "    #\n",
    "    labels.append(f'Maturity {vi_name} slope [1/d]')\n",
    "    Green_down = AVI_max - SVI_max\n",
    "    Green_down_err = additive_error(SVI_max_err, AVI_max_err)\n",
    "    Slope_of_Maturity = Green_down / DOM\n",
    "    values.append(Slope_of_Maturity)\n",
    "    Slope_of_Maturity_err = multiplicative_error(Slope_of_Maturity, Green_down, Green_down_err,\n",
    "                                                 DOM, DOM_err)\n",
    "    errors.append(Slope_of_Maturity_err)\n",
    "    #\n",
    "    labels.append(f'Autumn {vi_name} range')\n",
    "    AVI_range = AVI_max - VI_min\n",
    "    values.append(AVI_range)\n",
    "    AVI_range_err = additive_error(VI_min_err, AVI_max_err)\n",
    "    errors.append(AVI_range_err)\n",
    "    #\n",
    "    labels.append(f'Autumn {vi_name} inflection')\n",
    "    VI_AI = (VI_min + AVI_max) / 2.0\n",
    "    values.append(VI_AI)\n",
    "    VI_AI_err = AVI_range_err\n",
    "    errors.append(VI_AI_err)\n",
    "    #\n",
    "    labels.append(f'Autumn {vi_name} inflection [DOY]')\n",
    "    AI = (EOM + EOA) / 2.0\n",
    "    values.append(AI)\n",
    "    AI_err = additive_error(EOM_err, EOA_err)\n",
    "    errors.append(AI_err)\n",
    "    #\n",
    "    labels.append(f'Autumn {vi_name} duration [d]')\n",
    "    DOA = EOA - EOM\n",
    "    values.append(DOA)\n",
    "    DOA_err = AI_err\n",
    "    errors.append(DOA_err)\n",
    "    #\n",
    "    labels.append(f'Autumn {vi_name} slope [1/d]')\n",
    "    Slope_of_Autumn = AVI_range / DOA\n",
    "    values.append(Slope_of_Autumn)\n",
    "    Slope_of_Autumn_err = multiplicative_error(Slope_of_Autumn, AVI_range, AVI_range_err,\n",
    "                                               DOA, DOA_err)\n",
    "    errors.append(Slope_of_Autumn_err)\n",
    "    #\n",
    "    labels.append(f'Duration between {vi_name} inflections [d]')\n",
    "    D_SI_AI = AI - SI\n",
    "    values.append(D_SI_AI)\n",
    "    D_SI_AI_err = additive_error(SI_err, AI_err)\n",
    "    errors.append(D_SI_AI_err)\n",
    "    #\n",
    "    labels.append(f'Duration of {vi_name} season [d]')\n",
    "    D_SOS_EOA = EOA - SOS\n",
    "    values.append(D_SOS_EOA)\n",
    "    D_SOS_EOA_err = additive_error(SOS_err, EOA_err)\n",
    "    errors.append(D_SOS_EOA_err)\n",
    "    #\n",
    "    labels.append(f'Spring {vi_name} area [d]')\n",
    "    AOS = (SVI_range * DOS) / 2.0\n",
    "    values.append(AOS)\n",
    "    AOS_err = multiplicative_error(AOS, SVI_range, SVI_range_err, DOS, DOS_err)\n",
    "    errors.append(AOS_err)\n",
    "    #\n",
    "    labels.append(f'Maturity {vi_name} area [d]')\n",
    "    avg_VI_range = (SVI_range + AVI_range) / 2.0\n",
    "    avg_VI_range_err = additive_error(SVI_range_err, AVI_range_err)\n",
    "    AOM = DOM * avg_VI_range\n",
    "    values.append(AOM)\n",
    "    AOM_err = multiplicative_error(AOM, DOM, DOM_err, avg_VI_range, avg_VI_range_err)\n",
    "    errors.append(AOM_err)\n",
    "    #\n",
    "    labels.append(f'Autumn {vi_name} area [d]')\n",
    "    AOA = (AVI_range * DOA) / 2.0\n",
    "    values.append(AOA)\n",
    "    AOA_err = multiplicative_error(AOA, AVI_range, AVI_range_err, DOA, DOA_err)\n",
    "    errors.append(AOA_err)\n",
    "    #\n",
    "    labels.append(f'Total {vi_name} area [d]')\n",
    "    TA = AOS + AOM + AOA\n",
    "    values.append(TA)\n",
    "    TA_err = additive_error(AOM_err, additive_error(AOS_err, AOA_err))\n",
    "    errors.append(TA_err)\n",
    "    #\n",
    "    return labels, values, errors\n",
    "\n",
    "\n",
    "def get_vi_fit_metrics(metavals, pheno_fit_params_df):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    x_basis = metavals['x_basis']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    analysis_begin_year = metavals['analysis_begin_year']\n",
    "    analysis_end_year = metavals['analysis_end_year']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    n_outliers = metavals['n_outliers']\n",
    "    fname_tuple = (point_id, vi_name, curve_str, x_basis, outlier_round)\n",
    "    #\n",
    "    # get curve metrics\n",
    "    var_fit_params = pheno_fit_params_df[f'{vi_name}_by_{x_basis}']\n",
    "    labels, values, errors = calc_fitted_pheno_curve_metrics(vi_name, var_fit_params)\n",
    "    #\n",
    "    # report curve indicators and calculated metrics\n",
    "    print(f'Round {outlier_round}: {n_outliers} outliers identified')\n",
    "    print()\n",
    "    print(f'fitted {analysis_begin_year}-{analysis_end_year} mean {curve_str} curve metrics')\n",
    "    for l, label in enumerate(labels):\n",
    "        if (label[-1] == ']') and (label[-3] != '/'):\n",
    "            print(f'  {label} = {values[l]:.1f} +/- {errors[l]:.1f}')\n",
    "        else:\n",
    "            print(f'  {label} = {values[l]:.4f} +/- {errors[l]:.4f}')\n",
    "    print()\n",
    "    #\n",
    "    # create metrics dataframe \n",
    "    fit_metrics_df = pd.DataFrame({'label': labels, 'value_mean': values,\n",
    "                                   'error_mean': errors})\n",
    "    #\n",
    "    # save metrics dataframe to CSV\n",
    "    outfname = '%s_retained_%s_%s_%s_round_%d_fit_metrics.csv' % fname_tuple\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    fit_metrics_df.to_csv(outfpath, index=False)\n",
    "    print(f'saved {outfname}')\n",
    "    print()\n",
    "    #\n",
    "    return fit_metrics_df\n",
    "\n",
    "\n",
    "#\n",
    "# end part of phenology_model_vi_metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 18**: invoke function to calculate metrics for the long-term mean phenological curve\n",
    "\n",
    "Calls function in **Notebook Cell 17**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "\n",
    "\n",
    "pheno_fit_metrics_df = get_vi_fit_metrics(metavals, pheno_fit_params_df)\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploratory Analysis of Phenological Residuals**\n",
    "\n",
    "Various looks at the input variables, the VI residuals, and their properties:\n",
    "- trends in the response (VI residual) variable within all or part of the growing season (explored as correlations with DOY)\n",
    "- autocorrelation in the response (VI residual) variable\n",
    "- correlations between input (WxCD anomaly) and response (VI residual) variables\n",
    "- cross-correlation among input (WxCD anomaly) variables (collinearity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 19**: exploratory analysis of VI residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are the VI residuals correlated with DOY through the whole growing season?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    doys = np.array(pheno_fit_residuals_df['DOY'])\n",
    "    residual = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    #\n",
    "    min_val = np.min(doys)\n",
    "    max_val = np.max(doys)\n",
    "    regression_stats = regress(doys, residual)\n",
    "    x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "    y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "    #\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(doys, residual, marker='x', c='k', s=30)\n",
    "    plt.plot(x_regression, y_regression, 'b-', linewidth=2)\n",
    "    plt.xlim([metavals['growing_season_start'], metavals['growing_season_end']])\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('DOY', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    title = f'Full-year residuals (round {metavals[\"outlier_round\"]}):'\n",
    "    title += f' r = {regression_stats[2]:.4f}, p = {regression_stats[3]:.4f}'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 20**: exploratory analysis of VI residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are the VI residuals correlated with DOY during the (spring) green-up phase?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    var_fit_params = list(pheno_fit_metrics_df['value_mean'])\n",
    "    #\n",
    "    SOS = var_fit_params[1]\n",
    "    SOM = var_fit_params[3]\n",
    "    doys = np.array(pheno_fit_residuals_df['DOY'])\n",
    "    residuals = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    #\n",
    "    xvar = []\n",
    "    yvar = []\n",
    "    for d, doy in enumerate(doys):\n",
    "        if (doy > SOS) and (doy <= SOM):\n",
    "            xvar.append(doy)\n",
    "            yvar.append(residuals[d])\n",
    "    min_val = np.min(xvar)\n",
    "    max_val = np.max(xvar)\n",
    "    regression_stats = regress(xvar, yvar)\n",
    "    x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "    y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "    #\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(xvar, yvar, marker='x', c='k', s=30)\n",
    "    plt.plot(x_regression, y_regression, 'b-', linewidth=2)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('DOY', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    title = f'Green-up residuals (round {metavals[\"outlier_round\"]}):'\n",
    "    title += f' r = {regression_stats[2]:.4f}, p = {regression_stats[3]:.4f}'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 21**: exploratory analysis of VI residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are the VI residuals correlated with DOY during the (summer) mature phase?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    var_fit_params = list(pheno_fit_metrics_df['value_mean'])\n",
    "    #\n",
    "    SOM = var_fit_params[3]\n",
    "    EOM = var_fit_params[5]\n",
    "    doys = np.array(pheno_fit_residuals_df['DOY'])\n",
    "    residuals = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    #\n",
    "    xvar = []\n",
    "    yvar = []\n",
    "    for d, doy in enumerate(doys):\n",
    "        if (doy > SOM) and (doy < EOM):\n",
    "            xvar.append(doy)\n",
    "            yvar.append(residuals[d])\n",
    "    min_val = np.min(xvar)\n",
    "    max_val = np.max(xvar)\n",
    "    regression_stats = regress(xvar, yvar)\n",
    "    x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "    y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "    #\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(xvar, yvar, marker='x', c='k', s=30)\n",
    "    plt.plot(x_regression, y_regression, 'b-', linewidth=2)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('DOY', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    title = f'Maturity residuals (round {metavals[\"outlier_round\"]}):'\n",
    "    title += f' r = {regression_stats[2]:.4f}, p = {regression_stats[3]:.4f}'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 22**: exploratory analysis of VI residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are the VI residuals correlated with DOY during the (autumn) senescence phase?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    var_fit_params = list(pheno_fit_metrics_df['value_mean'])\n",
    "    #\n",
    "    EOM = var_fit_params[5]\n",
    "    EOA = var_fit_params[6]\n",
    "    doys = np.array(pheno_fit_residuals_df['DOY'])\n",
    "    residuals = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    #\n",
    "    xvar = []\n",
    "    yvar = []\n",
    "    for d, doy in enumerate(doys):\n",
    "        if (doy >= EOM) and (doy < EOA):\n",
    "            xvar.append(doy)\n",
    "            yvar.append(residuals[d])\n",
    "    min_val = np.min(xvar)\n",
    "    max_val = np.max(xvar)\n",
    "    regression_stats = regress(xvar, yvar)\n",
    "    x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "    y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "    #\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(xvar, yvar, marker='x', c='k', s=30)\n",
    "    plt.plot(x_regression, y_regression, 'b-', linewidth=2)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('DOY', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    title = f'Senescence residuals (round {metavals[\"outlier_round\"]}):'\n",
    "    title += f' r = {regression_stats[2]:.4f}, p = {regression_stats[3]:.4f}'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 23**: exploratory analysis of VI residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are the VI residuals correlated with the fitted-curve VI value through the whole growing season?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    xvar = np.array(pheno_fit_residuals_df['DOY'])\n",
    "    yvar = np.array(pheno_fit_residuals_df[f'{vi_name}_fitted'])\n",
    "    #\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(xvar, yvar, marker='x', c='k', s=30)\n",
    "    plt.xlim([metavals['growing_season_start'], metavals['growing_season_end']])\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('DOY', fontsize=11)\n",
    "    plt.ylabel(f'fitted {vi_name}', fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    #\n",
    "    xvar = np.array(pheno_fit_residuals_df[f'{vi_name}_fitted'])\n",
    "    yvar = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    min_val = np.min(xvar)\n",
    "    max_val = np.max(xvar)\n",
    "    regression_stats = regress(xvar, yvar)\n",
    "    x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "    y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "    #\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(xvar, yvar, marker='x', c='k', s=30)\n",
    "    plt.plot(x_regression, y_regression, 'b-', linewidth=2)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(f'fitted {vi_name}', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    title = f'Full-year residuals (round {metavals[\"outlier_round\"]}):'\n",
    "    title += f' r = {regression_stats[2]:.4f}, p = {regression_stats[3]:.4f}'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 24**: exploratory analysis of VI residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are the VI residuals correlated with the fitted-curve VI value during the (spring) green-up phase?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    var_fit_params = list(pheno_fit_metrics_df['value_mean'])\n",
    "    #\n",
    "    SOS = var_fit_params[1]\n",
    "    SOM = var_fit_params[3]\n",
    "    doys = np.array(pheno_fit_residuals_df['DOY'])\n",
    "    residuals = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    fitted = np.array(pheno_fit_residuals_df[f'{vi_name}_fitted'])\n",
    "    #\n",
    "    xvar = []\n",
    "    yvar = []\n",
    "    for d, doy in enumerate(doys):\n",
    "        if (doy > SOS) and (doy <= SOM):\n",
    "            xvar.append(fitted[d])\n",
    "            yvar.append(residuals[d])\n",
    "    min_val = np.min(xvar)\n",
    "    max_val = np.max(xvar)\n",
    "    regression_stats = regress(xvar, yvar)\n",
    "    x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "    y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "    #\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(xvar, yvar, marker='x', c='k', s=30)\n",
    "    plt.plot(x_regression, y_regression, 'b-', linewidth=2)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(f'fitted {vi_name}', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    title = f'Green-up residuals (round {metavals[\"outlier_round\"]}):'\n",
    "    title += f' r = {regression_stats[2]:.4f}, p = {regression_stats[3]:.4f}'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 25**: exploratory analysis of VI residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are the VI residuals correlated with the fitted-curve VI value during the (summer) mature phase?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    var_fit_params = list(pheno_fit_metrics_df['value_mean'])\n",
    "    #\n",
    "    SOM = var_fit_params[3]\n",
    "    EOM = var_fit_params[5]\n",
    "    doys = np.array(pheno_fit_residuals_df['DOY'])\n",
    "    residuals = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    fitted = np.array(pheno_fit_residuals_df[f'{vi_name}_fitted'])\n",
    "    #\n",
    "    xvar = []\n",
    "    yvar = []\n",
    "    for d, doy in enumerate(doys):\n",
    "        if (doy > SOM) and (doy < EOM):\n",
    "            xvar.append(fitted[d])\n",
    "            yvar.append(residuals[d])\n",
    "    min_val = np.min(xvar)\n",
    "    max_val = np.max(xvar)\n",
    "    regression_stats = regress(xvar, yvar)\n",
    "    x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "    y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "    #\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(xvar, yvar, marker='x', c='k', s=30)\n",
    "    plt.plot(x_regression, y_regression, 'b-', linewidth=2)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(f'fitted {vi_name}', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    title = f'Maturity residuals (round {metavals[\"outlier_round\"]}):'\n",
    "    title += f' r = {regression_stats[2]:.4f}, p = {regression_stats[3]:.4f}'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 26**: exploratory analysis of VI residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are the VI residuals correlated with the fitted-curve VI value during the (autumn) senescence phase?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    var_fit_params = list(pheno_fit_metrics_df['value_mean'])\n",
    "    #\n",
    "    EOM = var_fit_params[5]\n",
    "    EOA = var_fit_params[6]\n",
    "    doys = np.array(pheno_fit_residuals_df['DOY'])\n",
    "    residuals = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    fitted = np.array(pheno_fit_residuals_df[f'{vi_name}_fitted'])\n",
    "    #\n",
    "    xvar = []\n",
    "    yvar = []\n",
    "    for d, doy in enumerate(doys):\n",
    "        if (doy >= EOM) and (doy < EOA):\n",
    "            xvar.append(fitted[d])\n",
    "            yvar.append(residuals[d])\n",
    "    min_val = np.min(xvar)\n",
    "    max_val = np.max(xvar)\n",
    "    regression_stats = regress(xvar, yvar)\n",
    "    x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "    y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "    #\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(xvar, yvar, marker='x', c='k', s=30)\n",
    "    plt.plot(x_regression, y_regression, 'b-', linewidth=2)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(f'fitted {vi_name}', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    title = f'Senescence residuals (round {metavals[\"outlier_round\"]}):'\n",
    "    title += f'r = {regression_stats[2]:.4f} (p = {regression_stats[3]:.4f})'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 27**: exploratory analysis of VI residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Do the VI residuals show any patterns through the year that are common across all years?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    dates = np.array(pheno_fit_residuals_df['DYear'])\n",
    "    doys = np.array(pheno_fit_residuals_df['DOY'])\n",
    "    min_year = int(dates[0])\n",
    "    max_year = int(dates[-1]) + 1\n",
    "    residuals = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    #\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for year in range(min_year, max_year+1):\n",
    "        xvar = []\n",
    "        yvar = []\n",
    "        for d, dyear in enumerate(dates):\n",
    "            if int(dyear) == year:\n",
    "                xvar.append(doys[d])\n",
    "                yvar.append(residuals[d])\n",
    "        plt.plot(xvar, yvar, 'b-', marker='o', linewidth=1)\n",
    "    plt.xlim([metavals['growing_season_start'], metavals['growing_season_end']])\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('DOY', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 28**: exploratory analysis of VI residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Do the VI residuals in each year show any pattern across all years?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    dates = np.array(pheno_fit_residuals_df['DYear'])\n",
    "    min_year = int(dates[0])\n",
    "    max_year = int(dates[-1]) + 1\n",
    "    residuals = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    #\n",
    "    # detrend and fit a sine curve here \n",
    "    #\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for year in range(min_year, max_year+1):\n",
    "        xvar = []\n",
    "        yvar = []\n",
    "        for d, dyear in enumerate(dates):\n",
    "            if int(dyear) == year:\n",
    "                xvar.append(dyear)\n",
    "                yvar.append(residuals[d])\n",
    "        plt.plot(xvar, yvar, 'b-', marker='o', linewidth=1)\n",
    "    # add fitted sine curve\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('Year', fontsize=11)\n",
    "    plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 29**: exploratory analysis of the long-term mean phenological curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Is the VI fitted curve autocorrelated at any particular lag?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    coeff = []\n",
    "    lag_range = range(1, 31)\n",
    "    for lag in lag_range:\n",
    "        stats = regress(full_year_mean_vi[:-lag], full_year_mean_vi[lag:])\n",
    "        coeff.append(stats[2])\n",
    "    #\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(lag_range, coeff, 'b-', marker='o', linewidth=2)\n",
    "    plt.xlabel('lag [d]', fontsize=11)\n",
    "    plt.ylabel('r', fontsize=11)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 30**: exploratory analysis of VI residuals and the long-term mean phenological curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are the VI residuals correlated with the fitted curve at any particular lag?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    residual = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    coeff = []\n",
    "    pval = []\n",
    "    lag_range = range(91)\n",
    "    for lag in lag_range:\n",
    "        doys = np.array(pheno_fit_residuals_df['DOY'])\n",
    "        lagged_doys = doys - lag\n",
    "        lagged_fitted = [full_year_mean_vi[d-1] for d in lagged_doys]\n",
    "        stats = regress(residual, lagged_fitted)\n",
    "        coeff.append(stats[2])\n",
    "        pval.append(stats[3])\n",
    "    #\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(lag_range, coeff, 'b-', marker='o', linewidth=2)\n",
    "    plt.xlabel(f'lagged fitted {vi_name} [d]', fontsize=11)\n",
    "    plt.ylabel('r', fontsize=11)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    #\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(lag_range, pval, 'b-', marker='o', linewidth=2)\n",
    "    plt.xlabel(f'lagged fitted {vi_name} [d]', fontsize=11)\n",
    "    plt.ylabel('p', fontsize=11)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 31**: exploratory analysis of correlations between VI residuals and WxCD anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are any particular WxCD anomalies correlated with the VI residuals?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    x_basis = metavals['x_basis']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    fname_tuple = (point_id, vi_name, curve_str, x_basis, outlier_round)\n",
    "    #\n",
    "    slope = []\n",
    "    intercept = []\n",
    "    correlation = []\n",
    "    significance = []\n",
    "    marker = []\n",
    "    #\n",
    "    # get VI residuals\n",
    "    residual = np.array(pheno_fit_residuals_df[f'{vi_name}_residual'])\n",
    "    #\n",
    "    for v, var in enumerate(wxcd_std_anom_vars):\n",
    "        wxcd_stnd_anom = np.array(wxcd_vi_df[var])\n",
    "        regression_stats = regress(wxcd_stnd_anom, residual)\n",
    "        slope.append(regression_stats[0])\n",
    "        intercept.append(regression_stats[1])\n",
    "        correlation.append(regression_stats[2])\n",
    "        significance.append(regression_stats[3])\n",
    "        if regression_stats[3] < 0.000001:\n",
    "            marker.append('******')\n",
    "        elif regression_stats[3] < 0.00001:\n",
    "            marker.append('*****')\n",
    "        elif regression_stats[3] < 0.0001:\n",
    "            marker.append('****')\n",
    "        elif regression_stats[3] < 0.001:\n",
    "            marker.append('***')\n",
    "        elif regression_stats[3] < 0.01:\n",
    "            marker.append('**')\n",
    "        elif regression_stats[3] < 0.05:\n",
    "            marker.append('*')\n",
    "        else:\n",
    "            marker.append(' ')\n",
    "    #\n",
    "    wxcd_corr_df = pd.DataFrame({'variable': wxcd_std_anom_vars, 'marker': marker,\n",
    "                                 'slope': slope, 'intercept': intercept,\n",
    "                                 'corr': correlation, 'sig': significance})\n",
    "    print(f'Round {outlier_round} {vi_name} residuals vs. WxCD standardized anomalies')\n",
    "    wxcd_corr_df.sort_values(by='sig', ascending=True, inplace=True)\n",
    "    wxcd_corr_df = wxcd_corr_df.reset_index(drop=True)\n",
    "    print(wxcd_corr_df)\n",
    "    print()\n",
    "    #\n",
    "    outfname = '%s_retained_%s_%s_%s_round_%d_residual_WxCD_regressions.csv' % fname_tuple\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    wxcd_corr_df.to_csv(outfpath)\n",
    "    print(f'saved {outfname}')\n",
    "    print()\n",
    "    #\n",
    "    # get WxCD anomaly values and regress/plot against residuals\n",
    "    plot_width = 4\n",
    "    plot_height = 3\n",
    "    fig_ncols = 3\n",
    "    fig_width = plot_width * fig_ncols\n",
    "    fig_nrows = int(np.ceil(len(wxcd_std_anom_vars) / fig_ncols))\n",
    "    fig_height = plot_height * fig_nrows\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "    #\n",
    "    wxcd_vars_ordered = list(wxcd_corr_df['variable'])\n",
    "    for v, var in enumerate(wxcd_vars_ordered):\n",
    "        position = v + 1\n",
    "        # print(f'{position}', end=' ')\n",
    "        wxcd_stnd_anom = np.array(wxcd_vi_df[var])\n",
    "        min_val = np.min(wxcd_stnd_anom)\n",
    "        max_val = np.max(wxcd_stnd_anom)\n",
    "        regression_stats = regress(wxcd_stnd_anom, residual)\n",
    "        x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "        y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "        #\n",
    "        if regression_stats[3] <= 0.05:\n",
    "            plt.subplot(fig_nrows, fig_ncols, position, facecolor='lightgray')\n",
    "            plt.scatter(wxcd_stnd_anom, residual, marker='x', c='k', s=30)\n",
    "            plt.plot(x_regression, y_regression, 'r-', linewidth=2)\n",
    "        else:\n",
    "            plt.subplot(fig_nrows, fig_ncols, position)\n",
    "            plt.scatter(wxcd_stnd_anom, residual, marker='x', c='k', s=30)\n",
    "            plt.plot(x_regression, y_regression, 'b-', linewidth=2)\n",
    "        plt.xticks(fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.xlabel(var, fontsize=11)\n",
    "        plt.ylabel(f'{vi_name} residual', fontsize=11)\n",
    "        plt.title(f'r = {regression_stats[2]:.4f}, p = {regression_stats[3]:.4f}')\n",
    "        plt.tight_layout()\n",
    "    #\n",
    "    plotfname = '%s_retained_%s_%s_%s_round_%d_residual_WxCD_regressions.png' % fname_tuple\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    print()\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 32**: exploratory analysis of cross-correlations among WxCD anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Are any particular WxCD anomalies highly cross-correlated?\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    #\n",
    "    wxcd_stnd_anom_df = wxcd_vi_df.loc[:, wxcd_std_anom_vars]\n",
    "    wxcd_stnd_anom_corr_df = wxcd_stnd_anom_df.corr()\n",
    "    #\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    heatmap = sns.heatmap(wxcd_stnd_anom_corr_df, vmin=-1, vmax=1, annot=False, cmap='BrBG')\n",
    "    # heatmap.set_title(f'WxCD Cross-correlations at {point_id}', fontdict={'fontsize':12}, pad=12)\n",
    "    #\n",
    "    plotfname = f'{point_id}_all_WxCD_heatmap.png'\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 33**: plot selected WxCD values and anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not yet part of phenology_model_plots.py\n",
    "#\n",
    "\n",
    "\n",
    "def plot_wxcd_var_by_dyear(var_obs_name, units, var_dyears, var_obs_values,\n",
    "                           var_std_anom_name, var_std_anom_values,\n",
    "                           begin_year, end_year, output_path, point_id):\n",
    "    #\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    #\n",
    "    # plot WxCD values on Landsat dates by DYear\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    plt.scatter(var_dyears, var_obs_values, marker='x', c='k', s=30)\n",
    "    plt.xlim([begin_year, end_year+1])\n",
    "    years = np.arange(begin_year, end_year+2)\n",
    "    even_years = [year if year % 2 == 0 else '' for year in years]\n",
    "    plt.xticks(years, fontsize=10)\n",
    "    ax.set_xticklabels(even_years, fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('Year', fontsize=11)\n",
    "    plt.ylabel(f'{var_obs_name} [{units}]', fontsize=11)\n",
    "    title = f'{begin_year}-{end_year} {var_obs_name} observations'\n",
    "    plt.title(title, fontsize=12)\n",
    "    #\n",
    "    # plot WxCD anomalies on Landsat dates by DYear\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    plt.scatter(var_dyears, var_std_anom_values, marker='x', c='k', s=30)\n",
    "    plt.plot([begin_year, end_year+1], [0.0, 0.0], 'b-', linewidth=2)\n",
    "    plt.xlim([begin_year, end_year+1])\n",
    "    years = np.arange(begin_year, end_year+2)\n",
    "    even_years = [year if year % 2 == 0 else '' for year in years]\n",
    "    plt.xticks(years, fontsize=10)\n",
    "    ax.set_xticklabels(even_years, fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('Year', fontsize=11)\n",
    "    plt.ylabel(var_std_anom_name, fontsize=11)\n",
    "    title = f'{begin_year}-{end_year} {var_obs_name} standardized anomalies'\n",
    "    plt.title(title, fontsize=12)\n",
    "    #\n",
    "    plt.tight_layout()\n",
    "    plotfname = f'{point_id}_{var_obs_name}_observations+std_anoms_by_dyear.png'\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    #\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 34**: plot three specific WxCD variables (values and anomalies) as examples\n",
    "\n",
    "Calls function in **Notebook Cell 33**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some specific WxCD observations and climatological anomalies\n",
    "#\n",
    "\n",
    "if metavals['graph_output'] and metavals['exploratory']:\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    begin_year = metavals['analysis_begin_year']\n",
    "    end_year = metavals['analysis_end_year']\n",
    "    #\n",
    "    # get and plot a significantly correlated prcp variable\n",
    "    var_obs_name = 'prcp_365d_sum'\n",
    "    var_std_anom_name = 'prcp_365d_std_anom'\n",
    "    var_dyears = np.array(wxcd_vi_df['DYear'])\n",
    "    var_obs_values = np.array(wxcd_vi_df[var_obs_name])\n",
    "    var_std_anom_values = np.array(wxcd_vi_df[var_std_anom_name])\n",
    "    plot_wxcd_var_by_dyear(var_obs_name, 'mm', var_dyears, var_obs_values,\n",
    "                           var_std_anom_name, var_std_anom_values,\n",
    "                           begin_year, end_year, output_path, point_id)\n",
    "    #\n",
    "    # get and plot a significantly correlated tmin/tmax variable\n",
    "    var_obs_name = 'tmin_45d_avg'\n",
    "    var_std_anom_name = 'tmin_45d_std_anom'\n",
    "    var_dyears = np.array(wxcd_vi_df['DYear'])\n",
    "    var_obs_values = np.array(wxcd_vi_df[var_obs_name])\n",
    "    var_std_anom_values = np.array(wxcd_vi_df[var_std_anom_name])\n",
    "    plot_wxcd_var_by_dyear(var_obs_name, r'$^{\\circ}$C', var_dyears, var_obs_values,\n",
    "                           var_std_anom_name, var_std_anom_values,\n",
    "                           begin_year, end_year, output_path, point_id)\n",
    "    #\n",
    "    # get and plot an uncorrelated variable\n",
    "    var_obs_name = 'tavg_10d_avg'\n",
    "    var_std_anom_name = 'tavg_10d_std_anom'\n",
    "    var_dyears = np.array(wxcd_vi_df['DYear'])\n",
    "    var_obs_values = np.array(wxcd_vi_df[var_obs_name])\n",
    "    var_std_anom_values = np.array(wxcd_vi_df[var_std_anom_name])\n",
    "    plot_wxcd_var_by_dyear(var_obs_name, r'$^{\\circ}$C', var_dyears, var_obs_values,\n",
    "                           var_std_anom_name, var_std_anom_values,\n",
    "                           begin_year, end_year, output_path, point_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PLS Regression on Phenological Residuals**\n",
    "\n",
    "*Description and details here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 35**: statistical calculations to de-trend and re-trend a time series variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of phenology_model_stats.py\n",
    "#\n",
    "\n",
    "\n",
    "def detrend_given_stats(x, y, r_params):\n",
    "    slope, intercept = r_params[:2]\n",
    "    detrended_y = y - (slope * x + intercept)\n",
    "    #\n",
    "    return detrended_y\n",
    "\n",
    "\n",
    "def detrend(x, y):\n",
    "    r_params = regress(x, y)\n",
    "    detrended_y = detrend_given_stats(x, y, r_params)\n",
    "    #\n",
    "    return r_params, detrended_y\n",
    "\n",
    "\n",
    "def retrend(x, detrended_y, r_params):\n",
    "    slope, intercept = r_params[:2]\n",
    "    y = detrended_y + (slope * x + intercept)\n",
    "    #\n",
    "    return y\n",
    "\n",
    "\n",
    "#\n",
    "# end part of phenology_model_stats.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 36**: statistical calculations to standardize (center and scale) and de-standardize a time series variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of phenology_model_stats.py\n",
    "#\n",
    "\n",
    "\n",
    "def standardize_given_stats(y, y_stats):\n",
    "    y_mean, y_std = y_stats\n",
    "    if y_std:\n",
    "        standardized_y = (y - y_mean) / y_std\n",
    "    else:\n",
    "        standardized_y = np.zeros_like(y)\n",
    "    #\n",
    "    return standardized_y\n",
    "\n",
    "\n",
    "def standardize(y):\n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "    standardized_y = standardize_given_stats(y, [y_mean, y_std])\n",
    "    #\n",
    "    return [y_mean, y_std], standardized_y\n",
    "\n",
    "\n",
    "def destandardize(standardized_y, y_stats):\n",
    "    y_mean, y_std = y_stats\n",
    "    y = (standardized_y * y_std) + y_mean\n",
    "    #\n",
    "    return y\n",
    "\n",
    "\n",
    "#\n",
    "# end part of phenology_model_stats.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 37**: prepare input VI and WxCD variables (time series) for PLS regression\n",
    "\n",
    "Calls functions in **Notebook Cells 35 and 36**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# contents of phenology_model_plsr_prep.py\n",
    "#\n",
    "\n",
    "\n",
    "def prep_vars(vi_name, wxcd_vars, wxcd_vi_df, pheno_fit_anoms_df):\n",
    "    #\n",
    "    # set up output data structures\n",
    "    trend_params_cols = ['parameter', f'{vi_name}_fitted', f'{vi_name}_residual']\n",
    "    for var in wxcd_vars:\n",
    "        trend_params_cols.append(var)\n",
    "    trend_params_df = pd.DataFrame(columns=trend_params_cols)\n",
    "    trend_params_df['parameter'] = ['trend_slope', 'trend_intercept',\n",
    "                                    'trend_corr', 'trend_sig', 'trend_stderr']\n",
    "    #\n",
    "    stnd_params_cols = ['parameter', f'{vi_name}_fitted_detrended',\n",
    "                        f'{vi_name}_residual_detrended']\n",
    "    for var in wxcd_vars:\n",
    "        stnd_params_cols.append(f'{var}_detrended')\n",
    "    stnd_params_df = pd.DataFrame(columns=stnd_params_cols)\n",
    "    stnd_params_df['parameter'] = ['mean', 'std']\n",
    "    #\n",
    "    # detrend and standardize fitted VI values\n",
    "    dyear = np.array(pheno_fit_anoms_df['DYear'])\n",
    "    vi_fitted = np.array(pheno_fit_anoms_df[f'{vi_name}_fitted'])\n",
    "    detrend_params, vi_fitted_detrended = detrend(dyear, vi_fitted)\n",
    "    trend_params_df[f'{vi_name}_fitted'] = detrend_params\n",
    "    pheno_fit_anoms_df[f'{vi_name}_fitted_detrended'] = vi_fitted_detrended\n",
    "    stnd_params, vi_fitted_detrended_stnd = standardize(vi_fitted_detrended)\n",
    "    stnd_params_df[f'{vi_name}_fitted_detrended'] = stnd_params\n",
    "    pheno_fit_anoms_df[f'{vi_name}_fitted_detrended_stnd'] = vi_fitted_detrended_stnd\n",
    "    #\n",
    "    # detrend and standardize residual VI values\n",
    "    vi_residual = np.array(pheno_fit_anoms_df[f'{vi_name}_residual'])\n",
    "    detrend_params, vi_residual_detrended = detrend(dyear, vi_residual)\n",
    "    trend_params_df[f'{vi_name}_residual'] = detrend_params\n",
    "    pheno_fit_anoms_df[f'{vi_name}_residual_detrended'] = vi_residual_detrended\n",
    "    stnd_params, vi_residual_detrended_stnd = standardize(vi_residual_detrended)\n",
    "    stnd_params_df[f'{vi_name}_residual_detrended'] = stnd_params\n",
    "    pheno_fit_anoms_df[f'{vi_name}_residual_detrended_stnd'] = vi_residual_detrended_stnd\n",
    "    #\n",
    "    # obtain/calculate WxCD anomalies for retained VI dates\n",
    "    clim_stnd_anoms_cols = deepcopy(wxcd_vars)\n",
    "    for var in wxcd_vars:\n",
    "        clim_stnd_anoms_cols.append(f'{var}_detrended')\n",
    "        clim_stnd_anoms_cols.append(f'{var}_detrended_stnd')\n",
    "    #\n",
    "    clim_stnd_anoms_df = pd.DataFrame(columns=clim_stnd_anoms_cols)\n",
    "    for var in wxcd_vars:\n",
    "        clim_stnd_anoms_df[var] = np.array(wxcd_vi_df[var])\n",
    "    #\n",
    "    # detrend and standardize WxCD anomaly values\n",
    "    for var in wxcd_vars:\n",
    "        wxcd_stnd_anom = np.array(clim_stnd_anoms_df[var])\n",
    "        detrend_params, wxcd_stnd_anom_detrended = detrend(dyear, wxcd_stnd_anom)\n",
    "        trend_params_df[var] = detrend_params\n",
    "        clim_stnd_anoms_df[f'{var}_detrended'] = wxcd_stnd_anom_detrended\n",
    "        stnd_params, wxcd_stnd_anom_detrended_stnd = standardize(wxcd_stnd_anom_detrended)\n",
    "        stnd_params_df[f'{var}_detrended'] = stnd_params\n",
    "        clim_stnd_anoms_df[f'{var}_detrended_stnd'] = wxcd_stnd_anom_detrended_stnd\n",
    "    #\n",
    "    # concatenate the VI (pheno_fit_anoms) and WxCD (clim_stnd_anoms) dataframes\n",
    "    pheno_fit_anoms_df = pd.concat([pheno_fit_anoms_df, clim_stnd_anoms_df], axis=1)\n",
    "    #\n",
    "    return pheno_fit_anoms_df, trend_params_df, stnd_params_df\n",
    "\n",
    "\n",
    "def plsr_prep(metavals, wxcd_vars, wxcd_vi_retained_df, pheno_fit_anoms_df):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    x_basis = metavals['x_basis']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    fname_tuple = (point_id, vi_name, curve_str, x_basis, outlier_round)\n",
    "    #\n",
    "    print(f'Preparing round {outlier_round} {vi_name} residuals and WxCD for PLS regression')\n",
    "    print()\n",
    "    #\n",
    "    pheno_fit_anoms_df, trend_params_df, stnd_params_df = \\\n",
    "        prep_vars(vi_name, wxcd_vars, wxcd_vi_retained_df, pheno_fit_anoms_df)\n",
    "    #\n",
    "    # save retained detrended and standardized VI and WxCD anomalies\n",
    "    outfname = '%s_pls_retained_%s_%s_%s_round_%d_fit_wxcd_stnd_anoms.csv' % fname_tuple\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    pheno_fit_anoms_df.to_csv(outfpath, index=False)\n",
    "    print(f'saved {outfname}')\n",
    "    #\n",
    "    # save trend parameters for all vars (VI and WxCD)\n",
    "    outfname = '%s_pls_retained_%s_%s_%s_round_%d_trend_params.csv' % fname_tuple\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    trend_params_df.to_csv(outfpath, index=False)\n",
    "    print(f'saved {outfname}')\n",
    "    #\n",
    "    # save standardization parameters for all vars (VI and WxCD)\n",
    "    outfname = '%s_pls_retained_%s_%s_%s_round_%d_stnd_params.csv' % fname_tuple\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    stnd_params_df.to_csv(outfpath, index=False)\n",
    "    print(f'saved {outfname}')\n",
    "    print()\n",
    "    #\n",
    "    return trend_params_df, stnd_params_df, pheno_fit_anoms_df\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_plsr_prep.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 38**: invoke variable preparation prior to PLS regression\n",
    "\n",
    "Calls function in **Notebook Cell 37**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "\n",
    "\n",
    "print('PLS Regression for Parameter Reduction')\n",
    "print()\n",
    "#\n",
    "# setup output data structure\n",
    "plsr_results = {'point_id': metavals['point_id'],\n",
    "                'vi_name': metavals['vi_name'],\n",
    "                'curve_str': metavals['vi_curve_str'],\n",
    "                'x_basis': metavals['x_basis'],\n",
    "                'vip_threshold': metavals['vip_threshold']}\n",
    "#\n",
    "# prepare variables\n",
    "trend_params_df, stnd_params_df, pheno_fit_residuals_df = \\\n",
    "    plsr_prep(metavals, wxcd_std_anom_vars, wxcd_vi_df, pheno_fit_residuals_orig_df)\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 39**: actual PLS regression and prediction, with VIP calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of phenology_model_plsr.py\n",
    "#\n",
    "# many thanks to Townsend Lab member Adam Chlus for the VIP calculation\n",
    "#\n",
    "# optimizing this one function for GPU would be IDEAL\n",
    "#\n",
    "\n",
    "\n",
    "def pls_prediction(x_train, y_train, x_test, n_components):\n",
    "    \"\"\"\n",
    "    Partial least-squares (PLS) regression and prediction process\n",
    "    - regress, predict, extract coefficients, and calculate VIP\n",
    "    \n",
    "    Help with SVD convergence error handling from\n",
    "    http://stackoverflow.com/questions/9155478/\n",
    "    \"\"\"\n",
    "    pls_model = PLSRegression(n_components=n_components)\n",
    "    try:\n",
    "        pls_model.fit(x_train, y_train)\n",
    "    except np.linalg.linalg.LinAlgError as err:\n",
    "        if 'SVD did not converge' in err.print:\n",
    "            raise ValueError('ERROR: PLS SVD did not converge')\n",
    "    y_train_pred = pls_model.predict(x_train)\n",
    "    y_test_pred = pls_model.predict(x_test)\n",
    "    #\n",
    "    # gather model coefficients and intercept\n",
    "    coeffs_list = pls_model.coef_[0]\n",
    "    coeffs_arr = np.array(coeffs_list)\n",
    "    intercept = pls_model.intercept_[0]\n",
    "    #\n",
    "    # calculate VIP statistics\n",
    "    vip_list = []\n",
    "    nvars = len(coeffs_list)\n",
    "    loadings = pls_model.y_loadings_\n",
    "    scores = pls_model.x_scores_\n",
    "    sum_squares = loadings**2 + scores**2\n",
    "    weights = pls_model.x_weights_\n",
    "    sum_weights = np.sum(weights**2, axis=0)\n",
    "    for i in range(nvars):      \n",
    "        var_importance = sum_squares * (weights[i]**2) / sum_weights \n",
    "        vip_val = np.sqrt(nvars * var_importance.sum() / sum_squares.sum())\n",
    "        vip_list.append(vip_val)\n",
    "    vip_arr = np.array(vip_list)\n",
    "    #\n",
    "    return y_train_pred, y_test_pred, coeffs_arr, intercept, vip_arr\n",
    "\n",
    "\n",
    "#\n",
    "# end part of phenology_model_plsr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 40**: plot error metrics for multiple iterations of the PLS regression and prediction procedure over component space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_plsr_plots.py\n",
    "#\n",
    "\n",
    "\n",
    "def plot_error_metric(vi_name, nc_min, nc, metric_arr, metric_name,\n",
    "                      outlier_round, passnum, nc_best=0):\n",
    "    nc_range = range(nc_min, nc_min + nc)\n",
    "    plt.plot(nc_range, metric_arr, 'b-', marker='o', linewidth=2)\n",
    "    if np.max(metric_arr) > 0.0 and np.min(metric_arr) < 0.0:\n",
    "        plt.plot(nc_range, np.zeros_like(nc_range), 'k--', linewidth=1)\n",
    "    plt.xlabel('PLS regression components', fontsize=11)\n",
    "    plt.ylabel(metric_name, fontsize=11)\n",
    "    if nc_range[-1] > 20:\n",
    "        plt.xticks(nc_range[::2], fontsize=10)\n",
    "    else:\n",
    "        plt.xticks(nc_range, fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    title = f'{vi_name} PLSR {metric_name} (round {outlier_round}, '\n",
    "    if nc_best:\n",
    "        title += f'pass {passnum}, best nc = {nc_best})'\n",
    "    else:\n",
    "        title += f'pass {passnum})'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()  \n",
    "    #\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_plsr_error_metrics(metavals, n_components_min, n_components_max,\n",
    "                            n_components_best, rsq, press, aicc, bias, mae, rmse,\n",
    "                            passnum):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    x_basis = metavals['x_basis']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    #\n",
    "    # build figure\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    #\n",
    "    # plot AICc vs n_components\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plot_error_metric(vi_name, n_components_min, n_components_max, aicc, 'AICc',\n",
    "                      outlier_round, passnum, n_components_best)\n",
    "    #\n",
    "    # plot Rsq vs n_components\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plot_error_metric(vi_name, n_components_min, n_components_max, rsq, 'Rsq',\n",
    "                      outlier_round, passnum)\n",
    "    #\n",
    "    # plot PRESS vs n_components\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plot_error_metric(vi_name, n_components_min, n_components_max, press, 'PRESS',\n",
    "                      outlier_round, passnum)\n",
    "    #\n",
    "    # plot Bias vs n_components\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plot_error_metric(vi_name, n_components_min, n_components_max, bias, 'Bias',\n",
    "                      outlier_round, passnum)\n",
    "    #\n",
    "    # plot MAE vs n_components\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plot_error_metric(vi_name, n_components_min, n_components_max, mae, 'MAE',\n",
    "                      outlier_round, passnum)\n",
    "    #\n",
    "    # plot RMSE vs n_components\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plot_error_metric(vi_name, n_components_min, n_components_max, rmse, 'RMSE',\n",
    "                      outlier_round, passnum)\n",
    "    #\n",
    "    fname_tuple = (point_id, vi_name, curve_str, x_basis, outlier_round)\n",
    "    plotfname = '%s_pls_retained_%s_%s_%s_round_%d_nc_metrics' % fname_tuple\n",
    "    plotfname += f'_pass_{passnum}.png'\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    print()\n",
    "    #\n",
    "    return\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_plsr_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 41**: iterate the PLS regression and prediction procedure to find the optimum number of model components\n",
    "\n",
    "Calls functions in **Notebook Cells 7, 39, and 40**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of phenology_model_plsr.py\n",
    "#\n",
    "\n",
    "\n",
    "def n_components_iteration(idx, x, y, n_components, test_split):\n",
    "    #\n",
    "    # Uses train_test_split function from sklearn.model_selection\n",
    "    #\n",
    "    n_obs, n_params = np.shape(x)\n",
    "    #\n",
    "    # split input data to train and test parts\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_split)\n",
    "    #\n",
    "    # run PLS regression with specified number of model components\n",
    "    y_train_pred, y_test_pred, coeffs, intercept, vip = \\\n",
    "        pls_prediction(x_train, y_train, x_test, n_components)\n",
    "    #\n",
    "    # calculate error metrics from PLS test result\n",
    "    y_res = y_test_pred - y_test\n",
    "    fit_error_metrics = calc_fit_error_metrics(n_obs, n_params+1, y_test, y_res)\n",
    "    # fit_error_metrics = [bias, mae, rmse, sse, rsq, aic, aicc, bic]\n",
    "    #\n",
    "    return fit_error_metrics\n",
    "\n",
    "\n",
    "def find_n_components(metavals, x, y, passnum):\n",
    "    \"\"\"\n",
    "    Find optimum number of model components (n_components at lowest AICc)\n",
    "        using cross-validation with given data train/test split fraction\n",
    "    \"\"\"\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    vi_name = metavals['vi_name']\n",
    "    nc_max = metavals['nc_max']\n",
    "    test_split = metavals['test_split']\n",
    "    n_iterations = metavals['n_iterations']\n",
    "    graph_output = metavals['graph_output']\n",
    "    #\n",
    "    print(f'Checking PLS component space for optimal {vi_name} regression model')\n",
    "    n_obs, n_params = np.shape(x)\n",
    "    print(f'  n_parameters = {n_params}')\n",
    "    n_obs_test = int(np.round(n_obs * test_split))\n",
    "    n_obs_train = n_obs - n_obs_test\n",
    "    print(f'  n_observations = {n_obs}')\n",
    "    print(f'    n_training_obs = {n_obs_train}')\n",
    "    print(f'    n_testing_obs = {n_obs_test}')\n",
    "    n_components_min = 1\n",
    "    n_components_max = min([n_params - 1, nc_max])\n",
    "    print(f'  max n_components = {n_components_max}')\n",
    "    n_components_range = range(n_components_min, n_components_max + 1)\n",
    "    #\n",
    "    bias_arr = np.zeros((n_iterations, len(n_components_range)))\n",
    "    mae_arr = np.zeros_like(bias_arr)\n",
    "    rmse_arr = np.zeros_like(bias_arr)\n",
    "    press_arr = np.zeros_like(bias_arr)\n",
    "    rsq_arr = np.zeros_like(bias_arr)\n",
    "    aic_arr = np.zeros_like(bias_arr)\n",
    "    aicc_arr = np.zeros_like(bias_arr)\n",
    "    bic_arr = np.zeros_like(bias_arr)\n",
    "    #\n",
    "    # loop through n_components\n",
    "    print('  n_components')\n",
    "    for n_components in n_components_range:\n",
    "        print(f'    {n_components}')\n",
    "        for idx in range(n_iterations):\n",
    "            fit_error_metrics = n_components_iteration(idx, x, y, n_components, test_split)\n",
    "            # fit_error_metrics = [bias, mae, rmse, sse, rsq, aic, aicc, bic]\n",
    "            nc = n_components - n_components_min\n",
    "            bias_arr[idx, nc] = fit_error_metrics[0]\n",
    "            mae_arr[idx, nc] = fit_error_metrics[1]\n",
    "            rmse_arr[idx, nc] = fit_error_metrics[2]\n",
    "            press_arr[idx, nc] = fit_error_metrics[3]\n",
    "            rsq_arr[idx, nc] = fit_error_metrics[4]\n",
    "            aic_arr[idx, nc] = fit_error_metrics[5]\n",
    "            aicc_arr[idx, nc] = fit_error_metrics[6]\n",
    "            bic_arr[idx, nc] = fit_error_metrics[7]\n",
    "    #\n",
    "    print('Results')\n",
    "    rsq_mean = np.mean(rsq_arr[:, :], axis=0)\n",
    "    n_components_best = n_components_min + np.argmax(rsq_mean)\n",
    "    print(f'  max bootstrap Rsq = {np.max(rsq_mean):.3f} with n_components = {n_components_best}')\n",
    "    #\n",
    "    press_mean = np.mean(press_arr[:, :], axis=0)\n",
    "    n_components_best = n_components_min + np.argmin(press_mean)\n",
    "    print(f'  min bootstrap PRESS = {np.min(press_mean):.1f} with n_components = {n_components_best}')\n",
    "    #\n",
    "    aicc_mean = np.mean(aicc_arr[:, :], axis=0)\n",
    "    n_components_best = n_components_min + np.argmin(aicc_mean)\n",
    "    print(f'  min bootstrap AICc = {np.min(aicc_mean):.1f} with n_components = {n_components_best}')\n",
    "    print(f'  n_components = {n_components_best}')\n",
    "    #\n",
    "    bias_mean = np.mean(bias_arr[:, :], axis=0)\n",
    "    mae_mean = np.mean(mae_arr[:, :], axis=0)\n",
    "    rmse_mean = np.mean(rmse_arr[:, :], axis=0)\n",
    "    #\n",
    "    if graph_output:\n",
    "        plot_plsr_error_metrics(metavals, n_components_min, n_components_max, n_components_best,\n",
    "                                rsq_mean, press_mean, aicc_mean, bias_mean, mae_mean, rmse_mean,\n",
    "                                passnum)\n",
    "    #\n",
    "    return n_components_best\n",
    "\n",
    "\n",
    "#\n",
    "# end part of phenology_model_plsr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 42**: invoke procedure to find the optimum number of PLS model components and get the corresponding model details\n",
    "\n",
    "Calls functions in **Notebook Cells 7, 39, and 41**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of phenology_model_plsr.py\n",
    "#\n",
    "\n",
    "\n",
    "def pls_regression(metavals, x, y, passnum, n_components=0, print_stats=False):\n",
    "    \"\"\"\n",
    "    Run PLS with specified number of model components and report stats.\n",
    "    If no n_components specified, find optimum value by cross-validation.\n",
    "    \"\"\"\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    vi_name = metavals['vi_name']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    #\n",
    "    n_obs, n_params = np.shape(x)\n",
    "    #\n",
    "    # if n_components not specified, find optimal value by cross-validation\n",
    "    if n_components == 0:\n",
    "        n_components = find_n_components(metavals, x, y, passnum)\n",
    "    #\n",
    "    # PLS regression with identified number of model components\n",
    "    _, y_pred, coeffs, intercept, vip = \\\n",
    "        pls_prediction(x, y, x, n_components=n_components)\n",
    "    #\n",
    "    # process PLS regression results and error metrics\n",
    "    y_res = y_pred - y\n",
    "    fit_error_metrics = calc_fit_error_metrics(n_obs, n_params+1, y, y_res)\n",
    "    # fit_error_metrics = [bias, mae, rmse, sse, rsq, aic, aicc, bic]\n",
    "    #\n",
    "    if print_stats:\n",
    "        print(f'Round {outlier_round}: PLS regression on {vi_name} residuals')\n",
    "        print(f'  n_observations = {n_obs}')\n",
    "        print(f'  n_parameters = {n_params}')\n",
    "        print(f'  n_components = {n_components}')\n",
    "        print(f'  Bias = {fit_error_metrics[0]:.3f}')\n",
    "        print(f'  MAE = {fit_error_metrics[1]:.3f}')\n",
    "        print(f'  RMSE = {fit_error_metrics[2]:.3f}')\n",
    "        print(f'  PRESS = {fit_error_metrics[3]:.1f}')\n",
    "        print(f'  Rsq = {fit_error_metrics[4]:.3f}')\n",
    "        print(f'  AIC = {fit_error_metrics[5]:.1f}')\n",
    "        print(f'  AICc = {fit_error_metrics[6]:.1f}')\n",
    "        print(f'  BIC = {fit_error_metrics[7]:.1f}')\n",
    "    #\n",
    "    return n_components, fit_error_metrics, coeffs, intercept, vip\n",
    "\n",
    "\n",
    "#\n",
    "# end part of phenology_model_plsr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 43**: invoke the procedure to build and optimize a PLS model, and identify the variables with VIP ≥ 1.0\n",
    "\n",
    "Calls function in **Notebook Cell 42**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of phenology_model_plsr.py\n",
    "# \n",
    "\n",
    "\n",
    "def vip_threshold_plsr(metavals, x, y, variables, passnum, n_components=0,\n",
    "                       vip_threshold=1.0, print_stats=False):\n",
    "    \"\"\"\n",
    "    VIP-based selection of the most important parameters (variables).\n",
    "    \"\"\"\n",
    "    print(f'Selecting parameters using VIP threshold = {vip_threshold:.2f}')\n",
    "    n_components, fit_error_metrics, betas, intercept, vip = \\\n",
    "        pls_regression(metavals, x, y, passnum, n_components=n_components,\n",
    "                       print_stats=print_stats)\n",
    "    top_var_idxs = []\n",
    "    best_vars = []\n",
    "    best_vip = []\n",
    "    best_beta = []\n",
    "    for k, var in enumerate(variables):\n",
    "        if abs(vip[k]) > vip_threshold:\n",
    "            top_var_idxs.append(k)\n",
    "            best_vars.append(var)\n",
    "            best_vip.append(vip[k])\n",
    "            best_beta.append(betas[k])\n",
    "    if vip_threshold > 0.0:\n",
    "        print(f'- VIP threshold includes {len(best_vars)} of {len(variables)} parameters')\n",
    "    vip_df = pd.DataFrame({'varname': best_vars, 'vip': best_vip, 'beta': best_beta})\n",
    "    print()\n",
    "    #\n",
    "    return vip_df, n_components, fit_error_metrics, betas, intercept, vip\n",
    "\n",
    "\n",
    "#\n",
    "# end part of phenology_model_plsr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 44**: compile information on the PLS model produced at a given step in the iterative process loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of phenology_model_plsr.py\n",
    "# \n",
    "\n",
    "\n",
    "def compile_step_results(plsr_results, step, vi_name, n_obs, outlier_round,\n",
    "                         n_outliers, wxcd_std_anom_vars, n_components, vip,\n",
    "                         betas, intercept, fit_error_metrics):\n",
    "    var_prefix = f'{vi_name}_round_{outlier_round}_{step}'\n",
    "    plsr_results[f'{var_prefix}_n_obs'] = n_obs\n",
    "    plsr_results[f'{var_prefix}_n_outliers'] = n_outliers\n",
    "    plsr_results[f'{var_prefix}_vars'] = wxcd_std_anom_vars\n",
    "    plsr_results[f'{var_prefix}_nvars'] = len(wxcd_std_anom_vars)\n",
    "    plsr_results[f'{var_prefix}_n_components'] = n_components\n",
    "    plsr_results[f'{var_prefix}_vip'] = vip\n",
    "    plsr_results[f'{var_prefix}_betas'] = betas\n",
    "    plsr_results[f'{var_prefix}_intercept'] = intercept\n",
    "    plsr_results[f'{var_prefix}_bias'] = fit_error_metrics[0]\n",
    "    plsr_results[f'{var_prefix}_mae'] = fit_error_metrics[1]\n",
    "    plsr_results[f'{var_prefix}_rmse'] = fit_error_metrics[2]\n",
    "    plsr_results[f'{var_prefix}_press'] = fit_error_metrics[3]\n",
    "    plsr_results[f'{var_prefix}_rsq'] = fit_error_metrics[4]\n",
    "    plsr_results[f'{var_prefix}_aic'] = fit_error_metrics[5]\n",
    "    plsr_results[f'{var_prefix}_aicc'] = fit_error_metrics[6]\n",
    "    plsr_results[f'{var_prefix}_bic'] = fit_error_metrics[7]    \n",
    "    #\n",
    "    return plsr_results\n",
    "\n",
    "\n",
    "#\n",
    "# end part of phenology_model_plsr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 45**: Step 1 in the iterative PLS modeling loop, using all available WxCD variables\n",
    "\n",
    "Calls functions in **Notebook Cells 42, 43, and 44**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_plsr_step_1.py\n",
    "#\n",
    "\n",
    "\n",
    "def get_wxcd_detrended_stnd(vi_name, wxcd_vars, pheno_fit_residuals_df):\n",
    "    dyear = np.array(pheno_fit_residuals_df['DYear'])\n",
    "    wxcd_vars = [v for v in wxcd_vars if vi_name not in v]\n",
    "    wxcd_detrended_stnd = np.zeros((len(dyear), len(wxcd_vars)+1))\n",
    "    for n, var in enumerate(wxcd_vars):\n",
    "        var_arr = np.array(pheno_fit_residuals_df[f'{var}_detrended_stnd'])\n",
    "        if var_arr.ndim > 1:\n",
    "            wxcd_detrended_stnd[:, n] = var_arr[:, 0]\n",
    "        else:\n",
    "            wxcd_detrended_stnd[:, n] = var_arr\n",
    "    wxcd_detrended_stnd[:, len(wxcd_vars)] = \\\n",
    "        np.array(pheno_fit_residuals_df[f'{vi_name}_fitted_detrended_stnd'])\n",
    "    wxcd_vars.append(f'{vi_name}_fitted')\n",
    "    #\n",
    "    return wxcd_detrended_stnd\n",
    "\n",
    "\n",
    "def get_vi_detrended_stnd(vi_name, pheno_fit_residuals_df):\n",
    "    vi_detrended_stnd = \\\n",
    "        np.array(pheno_fit_residuals_df[f'{vi_name}_residual_detrended_stnd'])\n",
    "    #\n",
    "    return vi_detrended_stnd\n",
    "\n",
    "\n",
    "def plsr_step_1(metavals, plsr_results, wxcd_std_anom_vars,\n",
    "                wxcd_detrended_stnd, vi_detrended_stnd):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    vi_name = metavals['vi_name']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    n_outliers = metavals['n_outliers']\n",
    "    #\n",
    "    print(f'Round {outlier_round}: {n_outliers} outliers identified')\n",
    "    print(f'Step 1: PLS regression on {vi_name} using all parameters')\n",
    "    print()\n",
    "    #\n",
    "    # find n_components\n",
    "    passnum = 1\n",
    "    n_components, fit_error_metrics, betas, intercept, vip = \\\n",
    "        pls_regression(metavals, wxcd_detrended_stnd, vi_detrended_stnd,\n",
    "                       passnum, n_components=0)\n",
    "    #\n",
    "    # use n_components to run PLS without VIP threshold\n",
    "    initial_vip_df, n_components, fit_error_metrics, betas, intercept, vip = \\\n",
    "        vip_threshold_plsr(metavals, wxcd_detrended_stnd, vi_detrended_stnd,\n",
    "                           wxcd_std_anom_vars, passnum, n_components=n_components,\n",
    "                           vip_threshold=0.0, print_stats=True)\n",
    "    # fit_error_metrics = [bias, mae, rmse, sse, rsq, aic, aicc, bic]\n",
    "    #\n",
    "    # compile initial results with full variable complement\n",
    "    n_obs = metavals['n_observations'] - metavals['n_outliers']\n",
    "    plsr_results = compile_step_results(plsr_results, 'initial', vi_name, n_obs,\n",
    "                                        outlier_round, n_outliers, wxcd_std_anom_vars,\n",
    "                                        n_components, vip, betas, intercept,\n",
    "                                        fit_error_metrics)\n",
    "    #\n",
    "    return plsr_results\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_plsr_step_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 46**: invoke Step 1 in the iterative PLS modeling loop\n",
    "\n",
    "Calls functions in **Notebook Cell 45**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "\n",
    "#\n",
    "# gather input and response variables\n",
    "wxcd_detrended_stnd = get_wxcd_detrended_stnd(vi_name, wxcd_std_anom_vars,\n",
    "                                              pheno_fit_residuals_df)\n",
    "vi_detrended_stnd = get_vi_detrended_stnd(vi_name, pheno_fit_residuals_df)\n",
    "#\n",
    "# PLS regression, Step 1\n",
    "plsr_results_1 = plsr_step_1(metavals, plsr_results, wxcd_std_anom_vars,\n",
    "                             wxcd_detrended_stnd, vi_detrended_stnd)\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 47**: Step 2 in the iterative PLS modeling loop, to find the most important WxCD variables\n",
    "\n",
    "Calls functions in **Notebook Cells 43 and 44**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_plsr_step_2.py\n",
    "#\n",
    "\n",
    "\n",
    "def plsr_step_2(metavals, plsr_results, wxcd_detrended_stnd, vi_detrended_stnd):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    vi_name = metavals['vi_name']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    vip_threshold = metavals['vip_threshold']\n",
    "    n_outliers = metavals['n_outliers']\n",
    "    #\n",
    "    print(f'Round {outlier_round}: {n_outliers} outliers identified')\n",
    "    print('Step 2: PLS regression to find the most important variables/parameters')\n",
    "    print()\n",
    "    #\n",
    "    # use n_components found above to run PLS with VIP threshold\n",
    "    wxcd_vars = plsr_results[f'{vi_name}_round_{outlier_round}_initial_vars']\n",
    "    n_components = plsr_results[f'{vi_name}_round_{outlier_round}_initial_n_components']\n",
    "    passnum = 1\n",
    "    intermediate_vip_df, n_components, fit_error_metrics, betas, intercept, vip = \\\n",
    "        vip_threshold_plsr(metavals, wxcd_detrended_stnd, vi_detrended_stnd, wxcd_vars,\n",
    "                           passnum, n_components=n_components, vip_threshold=vip_threshold,\n",
    "                           print_stats=False)\n",
    "    # fit_error_metrics = [bias, mae, rmse, sse, rsq, aic, aicc, bic]\n",
    "    #\n",
    "    # show top variable rankings (VIP and beta coefficients)\n",
    "    print(f'{vi_name} PLS regression parameters with VIP >= {vip_threshold:.2f}')\n",
    "    intermediate_vip_df.sort_values(by='vip', ascending=False, inplace=True)\n",
    "    intermediate_vip_df.reset_index(drop=True, inplace=True)\n",
    "    print(intermediate_vip_df)\n",
    "    print()\n",
    "    #\n",
    "    # compile intermediate results for retained variables\n",
    "    n_obs = metavals['n_observations'] - metavals['n_outliers']\n",
    "    plsr_results = compile_step_results(plsr_results, 'intermediate', vi_name,\n",
    "                                        n_obs, outlier_round, n_outliers, \n",
    "                                        list(intermediate_vip_df['varname']),\n",
    "                                        n_components,\n",
    "                                        list(intermediate_vip_df['vip']),\n",
    "                                        list(intermediate_vip_df['beta']),\n",
    "                                        intercept, fit_error_metrics)\n",
    "    #\n",
    "    return plsr_results\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_plsr_step_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 48**: invoke Step 2 in the iterative PLS modeling loop\n",
    "\n",
    "Calls function in **Notebook Cell 47**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "\n",
    "#\n",
    "# PLS regression, Step 2\n",
    "plsr_results_2 = plsr_step_2(metavals, plsr_results_1,\n",
    "                             wxcd_detrended_stnd, vi_detrended_stnd)\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 49**: Step 3 in the iterative PLS modeling loop, to find the PLS model using only the most important WxCD variables\n",
    "\n",
    "Calls functions in **Notebook Cells 42, 43 and 44**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_plsr_step_3.py\n",
    "#\n",
    "\n",
    "\n",
    "def reduce_wxcd_vars(metavals, plsr_results, vi_detrended_stnd, pheno_fit_residuals_df):\n",
    "    vi_name = metavals['vi_name']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    #\n",
    "    retained_wxcd_vars = plsr_results[f'{vi_name}_round_{outlier_round}_intermediate_vars']\n",
    "    retained_wxcd_detrended_stnd = \\\n",
    "        np.zeros((len(vi_detrended_stnd), len(retained_wxcd_vars)))\n",
    "    #\n",
    "    for n, var in enumerate(retained_wxcd_vars):\n",
    "        if vi_name in var:\n",
    "            var_name = f'{var}_detrended_stnd'\n",
    "        else:\n",
    "            var_name = f'{var}_detrended_stnd'\n",
    "        var_arr = np.array(pheno_fit_residuals_df[var_name])\n",
    "        if var_arr.ndim > 1:\n",
    "            retained_wxcd_detrended_stnd[:, n] = var_arr[:, 0]\n",
    "        else:\n",
    "            retained_wxcd_detrended_stnd[:, n] = var_arr\n",
    "    #\n",
    "    return retained_wxcd_vars, retained_wxcd_detrended_stnd\n",
    "    \n",
    "\n",
    "def plsr_step_3(metavals, plsr_results, pheno_fit_residuals_df, vi_detrended_stnd):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    vi_name = metavals['vi_name']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    vip_threshold = metavals['vip_threshold']\n",
    "    n_outliers = metavals['n_outliers']\n",
    "    #\n",
    "    print(f'Round {outlier_round}: {n_outliers} outliers identified')\n",
    "    print(f'Step 3: PLSR using only parameters with VIP >= {vip_threshold}')\n",
    "    print()\n",
    "    #\n",
    "    # reduce the variable set to just the retained parameters\n",
    "    retained_wxcd_vars, retained_wxcd_detrended_stnd = \\\n",
    "        reduce_wxcd_vars(metavals, plsr_results, vi_detrended_stnd, pheno_fit_residuals_df)\n",
    "    #\n",
    "    # find nc for the final set of retained variables\n",
    "    passnum = 2\n",
    "    n_components, fit_error_metrics, betas, intercept, vip = \\\n",
    "        pls_regression(metavals, retained_wxcd_detrended_stnd, vi_detrended_stnd, \n",
    "                       passnum, n_components=0)\n",
    "    #\n",
    "    # use nc found above to run PLS without VIP threshold\n",
    "    final_vip_df, n_components, fit_error_metrics, betas, intercept, vip = \\\n",
    "        vip_threshold_plsr(metavals, retained_wxcd_detrended_stnd,\n",
    "                           vi_detrended_stnd, retained_wxcd_vars, passnum,\n",
    "                           n_components=n_components, vip_threshold=0.0,\n",
    "                           print_stats=True)\n",
    "    # fit_error_metrics = [bias, mae, rmse, sse, rsq, aic, aicc, bic]\n",
    "    #\n",
    "    # compile final results with reduced variable complement\n",
    "    n_obs = metavals['n_observations'] - metavals['n_outliers']\n",
    "    plsr_results = compile_step_results(plsr_results, 'final', vi_name, n_obs,\n",
    "                                        outlier_round, n_outliers, \n",
    "                                        list(final_vip_df['varname']), \n",
    "                                        n_components, list(final_vip_df['vip']),\n",
    "                                        list(final_vip_df['beta']), intercept,\n",
    "                                        fit_error_metrics)\n",
    "    #\n",
    "    return plsr_results\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_plsr_step_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 50**: invoke Step 3 in the iterative PLS modeling loop\n",
    "\n",
    "Calls function in **Notebook Cell 49**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "\n",
    "#\n",
    "# PLS regression, Step 3\n",
    "plsr_results_3 = plsr_step_3(metavals, plsr_results_2,\n",
    "                             pheno_fit_residuals_df, vi_detrended_stnd)\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 51**: save the PLS modeling summary information for this round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_plsr_save_results.py\n",
    "#\n",
    "\n",
    "\n",
    "def save_plsr_results(metavals, plsr_results):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    x_basis = metavals['x_basis']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    fname_tuple = (point_id, vi_name, curve_str, x_basis, outlier_round)\n",
    "    #\n",
    "    outfname = '%s_pls_retained_%s_%s_%s_round_%d_results.csv' % fname_tuple\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    with open(outfpath, 'w') as outfile:\n",
    "        w = csv.writer(outfile)\n",
    "        for key, val in plsr_results.items():\n",
    "            w.writerow([key, val])\n",
    "    print(f'saved {outfname}')\n",
    "    print()\n",
    "    #\n",
    "    return\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_plsr_save_results.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 52**: invoke procedure to save the PLS modeling summary for this round\n",
    "\n",
    "Calls function in **Notebook Cell 51**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "\n",
    "#\n",
    "# save results\n",
    "save_plsr_results(metavals, plsr_results_3)\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PLS Prediction of Phenological Residuals**\n",
    "\n",
    "*Description and details here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 53**: plot PLS-based predictions of VI values against observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_plsp_plot.py\n",
    "#\n",
    "\n",
    "\n",
    "def pls_prediction_plot(vi_name, outlier_round, x_vals, y_vals, x_label, pred_type, title_label):\n",
    "    regression_stats = regress(x_vals, y_vals)\n",
    "    rsq = regression_stats[2]**2\n",
    "    plt.scatter(x_vals, y_vals, marker='x', c='k', s=30)\n",
    "    min_val = np.min([np.min(x_vals), np.min(y_vals)])\n",
    "    max_val = np.max([np.max(x_vals), np.max(y_vals)])\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=1)\n",
    "    x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "    y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "    plt.plot(x_regression, y_regression, 'b-', linewidth=2)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(f'{x_label} {vi_name}', fontsize=11)\n",
    "    plt.ylabel(f'Observed {vi_name}', fontsize=11)\n",
    "    title_str = f'{pred_type} {vi_name} by {title_label} '\n",
    "    title_str += f'(round {outlier_round}, n = {len(y_vals)}, Rsq = {rsq:.3f})'\n",
    "    plt.title(title_str, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    #\n",
    "    return\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_plsp_plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 54**: Step 4 in the iterative PLS modeling loop, to predict VI values using the parsimonious model\n",
    "\n",
    "Calls functions in **Notebook Cells 8, 35, 36, 39, and 53**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_plsp.py\n",
    "#\n",
    "\n",
    "\n",
    "def plsp_image_dates(metavals, plsr_results, pheno_fit_params_df,\n",
    "                     pheno_fit_residuals_df, trend_params_df, stnd_params_df):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    x_basis = metavals['x_basis']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    remove_outliers = metavals['remove_outliers']\n",
    "    n_outliers = metavals['n_outliers']\n",
    "    doymin = metavals['doy_min']\n",
    "    doymax = metavals['doy_max']\n",
    "    forecast_begin_year = metavals['forecast_begin_year']\n",
    "    graph_output = metavals['graph_output']\n",
    "    fname_tuple = (point_id, vi_name, curve_str, x_basis, outlier_round)\n",
    "    #\n",
    "    print(f'Round {outlier_round}: {n_outliers} outliers identified')\n",
    "    print('Step 4: PLS prediction on image dates')\n",
    "    print()\n",
    "    #\n",
    "    wxcd_reduced_vars = plsr_results[f'{vi_name}_round_{outlier_round}_final_vars']\n",
    "    wxcd_reduced_nvars = len(wxcd_reduced_vars)\n",
    "    n_components = plsr_results[f'{vi_name}_round_{outlier_round}_final_n_components']\n",
    "    print(f'Predicting {vi_name} on all image dates using fitted PLS model')\n",
    "    print(f'  ({wxcd_reduced_nvars} parameters, {n_components} components)')\n",
    "    print()\n",
    "    #\n",
    "    retained_dates = np.array(pheno_fit_residuals_df['Date'])\n",
    "    n_retained_dates = len(retained_dates)\n",
    "    retained_doys = np.array(pheno_fit_residuals_df['DOY'])\n",
    "    retained_dyears = np.array(pheno_fit_residuals_df['DYear'])\n",
    "    #\n",
    "    # prepare variables for PLS prediction\n",
    "    vi_retained = np.array(pheno_fit_residuals_df[vi_name])\n",
    "    vi_retained_detrended_stnd = np.array(pheno_fit_residuals_df[f'{vi_name}_residual_detrended_stnd'])\n",
    "    wxcd_retained_detrended_stnd = np.zeros((n_retained_dates, wxcd_reduced_nvars))\n",
    "    for n, var in enumerate(wxcd_reduced_vars):\n",
    "        if vi_name in var:\n",
    "            #\n",
    "            # get vi_fitted for retained dates (detrended and standardized)\n",
    "            var_arr = np.array(pheno_fit_residuals_df[f'{var}_detrended_stnd'])\n",
    "            wxcd_retained_detrended_stnd[:, n] = var_arr\n",
    "            #\n",
    "            # then get and process vi_fitted for all image dates\n",
    "            fitted_curve_params = np.array(pheno_fit_params_df[f'{vi_name}_by_{x_basis}'])\n",
    "            retained_doys_ext = np.append(retained_doys, [doymin, doymax])\n",
    "            vi_retained = get_abc_curve(retained_doys_ext, fitted_curve_params)\n",
    "            #\n",
    "            # detrend series using existing parameters\n",
    "            detrend_params = np.array(trend_params_df[var][:2])\n",
    "            vi_retained_detrended = detrend_given_stats(retained_dyears, vi_retained, detrend_params)\n",
    "            #\n",
    "            # standardize detrended series using existing parameters\n",
    "            stnd_params = np.array(stnd_params_df[f'{var}_detrended'])\n",
    "            vi_retained_detrended_stnd = standardize_given_stats(vi_retained_detrended, stnd_params)\n",
    "            wxcd_retained_detrended_stnd[:, n] = vi_retained_detrended_stnd\n",
    "        else:\n",
    "            #\n",
    "            # first gather detrended and standardized WxCD anoms for retained dates\n",
    "            var_arr = np.array(pheno_fit_residuals_df[f'{var}_detrended_stnd'])\n",
    "            if var_arr.ndim > 1:\n",
    "                wxcd_retained_detrended_stnd[:, n] = var_arr[:, 0]\n",
    "            else:\n",
    "                wxcd_retained_detrended_stnd[:, n] = var_arr\n",
    "            #\n",
    "            # then get and process WxCD anoms for all image dates\n",
    "            clim_anom = np.array(pheno_fit_residuals_df[var])\n",
    "            #\n",
    "            # detrend climatological residuals using existing parameters\n",
    "            detrend_params = np.array(trend_params_df[var][:2])\n",
    "            clim_anom_detrended = detrend_given_stats(retained_dyears, clim_anom, detrend_params)\n",
    "            #\n",
    "            # standardize detrended climatological residuals using existing parameters\n",
    "            stnd_params = np.array(stnd_params_df[f'{var}_detrended'])\n",
    "            clim_anom_detrended_stnd = standardize_given_stats(clim_anom_detrended, stnd_params)\n",
    "            wxcd_retained_detrended_stnd[:, n] = clim_anom_detrended_stnd\n",
    "    #\n",
    "    # PLS prediction based on regression model (derived above, repeating for accuracy)\n",
    "    vi_retained_detrended_stnd_predicted, _, coeffs, intercept, vip = \\\n",
    "        pls_prediction(wxcd_retained_detrended_stnd, vi_retained_detrended_stnd,\n",
    "                       wxcd_retained_detrended_stnd, n_components)\n",
    "    #\n",
    "    # destandardize + retrend retained predicted VI residuals\n",
    "    stnd_params = np.array(stnd_params_df[f'{vi_name}_residual_detrended'])\n",
    "    vi_retained_detrended_predicted = \\\n",
    "        destandardize(vi_retained_detrended_stnd_predicted, stnd_params)\n",
    "    detrend_params = np.array(trend_params_df[f'{vi_name}_residual'][:2])\n",
    "    vi_retained_predicted_residual = \\\n",
    "        retrend(retained_dyears, vi_retained_detrended_predicted, detrend_params)\n",
    "    #\n",
    "    # recombine retained predicted VI residuals with mean phenology curve\n",
    "    fitted_curve_params = np.array(pheno_fit_params_df[f'{vi_name}_by_{x_basis}'])\n",
    "    doys_ext = np.append(retained_doys, [doymin, doymax])\n",
    "    vi_retained_mean = get_abc_curve(doys_ext, fitted_curve_params)\n",
    "    vi_retained_predicted = vi_retained_mean + vi_retained_predicted_residual\n",
    "    col_name = f'{vi_name}_by_{x_basis}_predicted_round_{outlier_round}'\n",
    "    pheno_fit_residuals_df_new_cols = pd.DataFrame({col_name: vi_retained_predicted})\n",
    "    pheno_fit_residuals_df = pd.concat([pheno_fit_residuals_df, pheno_fit_residuals_df_new_cols], axis=1)\n",
    "    #\n",
    "    if graph_output:\n",
    "        if remove_outliers or forecast_begin_year:\n",
    "            plt.figure(figsize=(12, 12))\n",
    "        else:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "        #\n",
    "        # plot figure with retained observations vs mean-curve predictions\n",
    "        if remove_outliers or forecast_begin_year:\n",
    "            plt.subplot(2, 2, 1)\n",
    "        else:\n",
    "            plt.subplot(1, 2, 1)\n",
    "        pls_prediction_plot(vi_name, outlier_round, vi_retained_mean, vi_retained,\n",
    "                            'Phenological mean', 'Expected', 'mean curve')\n",
    "        #\n",
    "        # plot figure with retained observations vs predictions\n",
    "        if remove_outliers or forecast_begin_year:\n",
    "            plt.subplot(2, 2, 2)\n",
    "        else:\n",
    "            plt.subplot(1, 2, 2)\n",
    "        pls_prediction_plot(vi_name, outlier_round, vi_retained_predicted, vi_retained,\n",
    "                            'PLS-predicted', 'Expected', 'PLS')\n",
    "    #\n",
    "    # destandardize + retrend predicted VI residuals\n",
    "    stnd_params = np.array(stnd_params_df[f'{vi_name}_residual_detrended'])\n",
    "    vi_retained_detrended_predicted = destandardize(vi_retained_detrended_stnd_predicted, stnd_params)\n",
    "    detrend_params = np.array(trend_params_df[f'{vi_name}_residual'][:2])\n",
    "    vi_retained_predicted_residual = retrend(retained_dyears, vi_retained_detrended_predicted, detrend_params)\n",
    "    #\n",
    "    # recombine predicted VI residuals with base curve\n",
    "    fitted_curve_params = np.array(pheno_fit_params_df[f'{vi_name}_by_{x_basis}'])\n",
    "    retained_doys_ext = np.append(retained_doys, [doymin, doymax])\n",
    "    vi_retained_mean = get_abc_curve(retained_doys_ext, fitted_curve_params)\n",
    "    vi_retained_predicted = vi_retained_mean + vi_retained_predicted_residual\n",
    "    #\n",
    "    # expand VI dataframe with PLS-predicted values\n",
    "    pheno_fit_residuals_df[f'{vi_name}_mean'] = np.array(vi_retained_mean)\n",
    "    pheno_fit_residuals_df[f'{vi_name}_predicted'] = np.array(vi_retained_predicted)\n",
    "    #\n",
    "    if graph_output:\n",
    "        if forecast_begin_year:\n",
    "            #\n",
    "            # trim dataframe to forecast period\n",
    "            forecast_wxcd_vi_df = pheno_fit_residuals_df[pheno_fit_residuals_df['Year'] >= forecast_begin_year]        \n",
    "            vi_forecast = np.array(forecast_wxcd_vi_df[vi_name])\n",
    "            vi_forecast_mean = np.array(forecast_wxcd_vi_df[f'{vi_name}_mean'])\n",
    "            vi_forecast_predicted = np.array(forecast_wxcd_vi_df[f'{vi_name}_predicted'])\n",
    "            #\n",
    "            # plot forecast period figure with observations vs mean-curve predictions\n",
    "            plt.subplot(2, 2, 3)\n",
    "            pls_prediction_plot(vi_name, outlier_round, vi_forecast_mean, vi_forecast,\n",
    "                                'Phenological mean', 'Forecast', 'mean curve')\n",
    "            #\n",
    "            # plot forecast period figure with observations vs PLS predictions\n",
    "            plt.subplot(2, 2, 4)\n",
    "            pls_prediction_plot(vi_name, outlier_round, vi_forecast_predicted, vi_forecast,\n",
    "                                'PLS-predicted', 'Expected', 'PLS')\n",
    "            #\n",
    "            plotfname = '%s_%s_%s_%s_round_%d_pls_prediction+forecast.png' % fname_tuple\n",
    "            plotfpath = f'{output_path}/{plotfname}'\n",
    "            plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "            print(f'saved {plotfname}')\n",
    "        else:\n",
    "            plotfname = '%s_%s_%s_%s_round_%d_pls_prediction.png' % fname_tuple\n",
    "            plotfpath = f'{output_path}/{plotfname}'\n",
    "            plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "            print(f'saved {plotfname}')\n",
    "    #\n",
    "    # save expanded VI dataframe with PLS predicted values\n",
    "    #\n",
    "    outfname = '%s_pls_predicted_%s_%s_%s_round_%d.csv' % fname_tuple\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    pheno_fit_residuals_df.to_csv(outfpath, index=False)\n",
    "    print(f'saved {outfname}')\n",
    "    #\n",
    "    return pheno_fit_residuals_df\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_plsp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 55**: invoke procedure to predict VI values using the parsimonious model\n",
    "\n",
    "Calls function in **Notebook Cell 54**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "\n",
    "#\n",
    "# do PLS model-based prediction on image dates\n",
    "pheno_fit_residuals_df = plsp_image_dates(metavals, plsr_results_3, pheno_fit_params_df,\n",
    "                                          pheno_fit_residuals_df, trend_params_df, stnd_params_df)\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 56**: calculate and plot the statistical prediction interval for the parsimonious PLS-based model, and identify outlier observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_get_outliers.py\n",
    "#\n",
    "\n",
    "\n",
    "def calculate_prediction_interval(x, y, outlier_confidence):\n",
    "    #\n",
    "    # this function is based on\n",
    "    # https://github.com/demotu/BMC/blob/master/notebooks/CurveFitting.ipynb\n",
    "    #\n",
    "    n_obs = y.size\n",
    "    parameters, covariance = np.polyfit(x, y, 1, cov=True)\n",
    "    y_fit = np.polyval(parameters, x)\n",
    "    residuals = y - y_fit                           \n",
    "    n_params = parameters.size\n",
    "    degrees_of_freedom = n_obs - n_params\n",
    "    stdv_of_error = np.sqrt(np.sum(residuals**2) / degrees_of_freedom)\n",
    "    p_value = 1.0 - ((1.0 - outlier_confidence) / 2.0)\n",
    "    t_value = scipy.stats.t.ppf(p_value, degrees_of_freedom)\n",
    "    num = (x - np.mean(x))**2\n",
    "    denom = np.sum((x - np.mean(x))**2)\n",
    "    radicand = 1 + (1 / n_obs) + (num / denom)  # for prediction interval, not confidence interval\n",
    "    prediction_interval = t_value * stdv_of_error * np.sqrt(radicand)\n",
    "    #\n",
    "    return y_fit, prediction_interval\n",
    "\n",
    "\n",
    "def pls_prediction_interval_plot(metavals, vi_retained, vi_retained_predicted, retained_dates):\n",
    "    #\n",
    "    # this function is based on\n",
    "    # https://stackoverflow.com/questions/27164114/show-confidence-limits-and-prediction-limits-in-scatter-plot\n",
    "    # which is itself based on \n",
    "    # https://github.com/demotu/BMC/blob/master/notebooks/CurveFitting.ipynb    \n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    x_basis = metavals['x_basis']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    outlier_confidence = metavals['outlier_confidence']\n",
    "    graph_output = metavals['graph_output']\n",
    "    fname_tuple = (point_id, vi_name, curve_str, x_basis, outlier_round)\n",
    "    #\n",
    "    # sort the values on x to plot the prediction interval correctly\n",
    "    df = pd.DataFrame({'x': vi_retained_predicted,\n",
    "                       'y': vi_retained,\n",
    "                       'd': retained_dates})\n",
    "    df.sort_values(by=['x'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    x = np.array(df['x'])\n",
    "    y = np.array(df['y'])\n",
    "    dates = list(df['d'])\n",
    "    #\n",
    "    if graph_output:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        #\n",
    "        # plot measured and predicted VI for individual retained dates\n",
    "        plt.scatter(x, y, marker='x', c='k', s=30,\n",
    "                    label=f'Landsat scenes ({len(vi_retained)})')\n",
    "        #\n",
    "        # plot 1:1 line\n",
    "        min_val = np.min([np.min(x), np.min(y)])\n",
    "        max_val = np.max([np.max(x), np.max(y)])\n",
    "        plt.plot([min_val-0.02, max_val+0.02], [min_val-0.02, max_val+0.02], \n",
    "                 'k--', linewidth=1, label='1:1 line')\n",
    "        #\n",
    "        # plot regression line\n",
    "        regression_stats = regress(x, y)\n",
    "        rsq = regression_stats[2]**2\n",
    "        x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "        y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "        rsq_str = r'$r^2$'\n",
    "        plt.plot(x_regression, y_regression, 'b-', linewidth=2,\n",
    "                 label=f'linear regression ({rsq_str} = {rsq:.4f})')\n",
    "    #\n",
    "    # calculate and plot the prediction interval based on the stated outlier confidence\n",
    "    y_fit, prediction_interval = \\\n",
    "        calculate_prediction_interval(x, y, outlier_confidence)\n",
    "    if graph_output:\n",
    "        confidence_str = f'{(outlier_confidence * 100):.1f}%'\n",
    "        plt.fill_between(x, y_fit+prediction_interval, y_fit-prediction_interval,\n",
    "                         color='lightblue', alpha=0.5,\n",
    "                         label=f'{confidence_str} prediction interval')\n",
    "        #\n",
    "        # overplot measured and predicted VI for individual retained dates\n",
    "        plt.scatter(vi_retained_predicted, vi_retained, marker='x', c='k', s=30)\n",
    "    #\n",
    "    # identify outliers\n",
    "    outlier_x = []\n",
    "    outlier_y = []\n",
    "    outlier_dates = []\n",
    "    y_dist = vi_retained - y_fit\n",
    "    for i, x_i in enumerate(x):\n",
    "        y_i = y[i]\n",
    "        y_fit_i = y_fit[i]\n",
    "        dist = np.abs(y_i - y_fit_i)\n",
    "        pi_i = prediction_interval[i]\n",
    "        if dist > pi_i:\n",
    "            outlier_x.append(x_i)\n",
    "            outlier_y.append(y_i)\n",
    "            outlier_dates.append(dates[i])\n",
    "    if graph_output:\n",
    "        plt.scatter(outlier_x, outlier_y, marker='o', c='r', edgecolor='k', s=40,\n",
    "                    label=f'outlier scenes ({len(outlier_y)})')\n",
    "        #\n",
    "        plt.legend(loc='upper left')\n",
    "        #\n",
    "        plt.xticks(fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.xlabel(f'PLS-predicted {vi_name}', fontsize=11)\n",
    "        plt.ylabel(f'Observed {vi_name}', fontsize=11)\n",
    "        title = f'Round {outlier_round} predicted {vi_name} by PLS'\n",
    "        plt.title(title, fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        #\n",
    "        plotfname = '%s_%s_%s_%s_round_%d_pls_prediction_outliers.png' % fname_tuple\n",
    "        plotfpath = f'{output_path}/{plotfname}'\n",
    "        plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "        print(f'saved {plotfname}')\n",
    "    #\n",
    "    return outlier_dates\n",
    "    \n",
    "\n",
    "def find_outliers(metavals, pheno_fit_residuals_df, plot_outliers=True):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    vi_name = metavals['vi_name']\n",
    "    x_basis = metavals['x_basis']\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    #\n",
    "    # plot figure with retained observations vs predictions\n",
    "    vi_retained = np.array(pheno_fit_residuals_df[vi_name])\n",
    "    col_name = f'{vi_name}_by_{x_basis}_predicted_round_{outlier_round}'\n",
    "    vi_retained_predicted = np.array(pheno_fit_residuals_df[col_name])\n",
    "    retained_dates = np.array(pheno_fit_residuals_df['Date'])\n",
    "    #\n",
    "    if plot_outliers:\n",
    "        outlier_dates = pls_prediction_interval_plot(metavals, vi_retained,\n",
    "                                                     vi_retained_predicted,\n",
    "                                                     retained_dates) \n",
    "    #\n",
    "    return outlier_dates\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_get_outliers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 57**: invoke procedure to identify outlier observations using the statistical prediction interval for the parsimonious PLS-based model\n",
    "\n",
    "Calls function in **Notebook Cell 56**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "\n",
    "\n",
    "if metavals['remove_outliers']:\n",
    "    outlier_dates = find_outliers(metavals, pheno_fit_residuals_df)\n",
    "    print('End of Round %d: post-PLS outlier identification' % metavals['outlier_round'])\n",
    "    if outlier_dates:\n",
    "        confidence = metavals['outlier_confidence']\n",
    "        confidence_str = f'{(confidence * 100):.1f}%'\n",
    "        print(f'- found {len(outlier_dates)} outlier dates based on {confidence_str} prediction interval')\n",
    "        print(f'{str(outlier_dates)}')\n",
    "    else:\n",
    "        print('- no outliers found')\n",
    "else:\n",
    "    print('No outlier iterations requested')\n",
    "    metavals['optimal_outlier_round'] = 0\n",
    "print()\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 58**: plot a summary of PLS model statistics through several rounds of outlier removal and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of phenology_model_plsr_summary.py\n",
    "#\n",
    "\n",
    "\n",
    "def make_summary_plot(x_vals, y_vals, y_label, vi_name, title_end):\n",
    "    plt.plot(x_vals, y_vals, 'b-', marker='o', linewidth=2)\n",
    "    plt.xlabel('Outlier Round', fontsize=11)\n",
    "    plt.ylabel(y_label, fontsize=11)\n",
    "    plt.xticks(x_vals, fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.title(f'{vi_name} PLSR model summary {title_end}')\n",
    "    plt.tight_layout()\n",
    "    #\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_plsr_summary(metavals, plsr_results):\n",
    "    #\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    x_basis = metavals['x_basis']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    n_outlier_rounds = metavals['outlier_round']\n",
    "    fname_tuple = (point_id, vi_name, curve_str, x_basis)\n",
    "    outlier_rounds = np.arange(n_outlier_rounds+1)\n",
    "    #\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    #\n",
    "    plt.subplot(3, 3, 1)\n",
    "    round_final_n_obs = [plsr_results[f'{vi_name}_round_{r}_final_n_obs'] for r in outlier_rounds]\n",
    "    make_summary_plot(outlier_rounds, round_final_n_obs, 'n_observations remaining', vi_name, 'n_observations')\n",
    "    #\n",
    "    plt.subplot(3, 3, 2)\n",
    "    round_final_n_outliers = [plsr_results[f'{vi_name}_round_{r}_final_n_outliers'] for r in outlier_rounds]\n",
    "    make_summary_plot(outlier_rounds, round_final_n_outliers, 'total n_outliers identified', vi_name, 'n_outliers')\n",
    "    #\n",
    "    err_metrics = ['AICc', 'PRESS', 'Rsq', 'RMSE', 'MAE', 'Nvars', 'n_components']\n",
    "    for m, metric in enumerate(err_metrics):\n",
    "        plt.subplot(3, 3, m+3)\n",
    "        round_final_metric = [plsr_results[f'{vi_name}_round_{r}_final_{metric.lower()}'] for r in outlier_rounds]\n",
    "        make_summary_plot(outlier_rounds, round_final_metric, f'Round final {metric}', vi_name, metric)\n",
    "    #\n",
    "    plotfname = '%s_%s_%s_%s_pls_modeling_stats_summary.png' % fname_tuple\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    #\n",
    "    return\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_plsr_summary.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 59**: main modeling function to execute several rounds of outlier removal and PLS-based model optimization\n",
    "\n",
    "Calls functions in **Notebook Cells 14, 17, 37, 45, 47, 49, 51, 54, 56, and 58**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "row"
   },
   "outputs": [],
   "source": [
    "# contents of phenology_model_remove_outliers.py\n",
    "#\n",
    "\n",
    "\n",
    "def remove_outliers(metavals, outlier_dates, plsr_results, wxcd_vi_retained_df):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    x_basis = metavals['x_basis']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    remove_outliers = metavals['remove_outliers']\n",
    "    max_outlier_rounds = metavals['max_outlier_rounds']\n",
    "    graph_output = metavals['graph_output']\n",
    "    #\n",
    "    outlier_round = metavals['outlier_round']\n",
    "    if outlier_round >= metavals['max_outlier_rounds']:\n",
    "        iterate = False\n",
    "    else:\n",
    "        iterate = True\n",
    "    overall_aicc = [plsr_results[f'{vi_name}_round_{outlier_round}_final_aicc']]\n",
    "    disturb_dates_all = []\n",
    "    #\n",
    "    while iterate:\n",
    "        #\n",
    "        # pull needed metavals\n",
    "        outlier_round = metavals['outlier_round']\n",
    "        #\n",
    "        fname_tuple = (point_id, vi_name, curve_str, x_basis, outlier_round)\n",
    "        #\n",
    "        # concatenate outlier collections\n",
    "        disturb_dates_all = sorted(list(set(disturb_dates_all) | set(outlier_dates)))\n",
    "        #\n",
    "        # update metavals\n",
    "        metavals['n_outliers'] = len(disturb_dates_all)\n",
    "        #\n",
    "        # split dataset according to found outliers\n",
    "        wxcd_vi_outliers_df = wxcd_vi_retained_df[wxcd_vi_retained_df.Date.isin(outlier_dates)]\n",
    "        nrows, ncols = wxcd_vi_outliers_df.shape\n",
    "        print(f'outliers WxCD/VI data array has {nrows} rows, {ncols} cols')\n",
    "        #\n",
    "        # save outlier dates information\n",
    "        outfname = '%s_outliers_%s_%s_%s_round_%d.csv' % fname_tuple\n",
    "        outfpath = f'{output_path}/{outfname}'\n",
    "        wxcd_vi_outliers_df.to_csv(outfpath, index=False)\n",
    "        print(f'- saved {outfname}')\n",
    "        print()\n",
    "        #\n",
    "        # reduce dataset to retained dates\n",
    "        wxcd_vi_retained_df = wxcd_vi_retained_df[~wxcd_vi_retained_df.Date.isin(outlier_dates)]\n",
    "        nrows, ncols = wxcd_vi_retained_df.shape\n",
    "        print(f'retained WxCD/VI data array has {nrows} rows, {ncols} cols')\n",
    "        #\n",
    "        # save retained dates information\n",
    "        outfname = '%s_retained_%s_%s_%s_round_%d.csv' % fname_tuple\n",
    "        outfpath = f'{output_path}/{outfname}'\n",
    "        wxcd_vi_retained_df.to_csv(outfpath, index=False)\n",
    "        print(f'- saved {outfname}')\n",
    "        print()\n",
    "        #\n",
    "        # fit ABC curve\n",
    "        metavals['outlier_round'] += 1\n",
    "        pheno_fit_params_df, pheno_fit_residuals_df, full_year_mean_vi = \\\n",
    "            model_vi_pheno_curve(metavals, wxcd_vi_retained_df, wxcd_vi_outliers_df)\n",
    "        #\n",
    "        # get ABC curve metrics\n",
    "        pheno_fit_metrics_df = get_vi_fit_metrics(metavals, pheno_fit_params_df)\n",
    "        #\n",
    "        # PLSR prep\n",
    "        trend_params_df, stnd_params_df, pheno_fit_residuals_df = \\\n",
    "            plsr_prep(metavals, wxcd_std_anom_vars, wxcd_vi_retained_df, pheno_fit_residuals_df)\n",
    "        #\n",
    "        # gather input and response variables\n",
    "        wxcd_detrended_stnd = get_wxcd_detrended_stnd(vi_name, wxcd_std_anom_vars,\n",
    "                                                      pheno_fit_residuals_df)\n",
    "        vi_detrended_stnd = get_vi_detrended_stnd(vi_name, pheno_fit_residuals_df)        \n",
    "        #\n",
    "        # PLSR Step 1\n",
    "        plsr_results_1 = plsr_step_1(metavals, plsr_results, wxcd_std_anom_vars,\n",
    "                                     wxcd_detrended_stnd, vi_detrended_stnd)\n",
    "        #\n",
    "        # PLSR Step 2\n",
    "        plsr_results_2 = plsr_step_2(metavals, plsr_results_1, wxcd_detrended_stnd,\n",
    "                                     vi_detrended_stnd)\n",
    "        #\n",
    "        # PLSR Step 3\n",
    "        plsr_results_3 = plsr_step_3(metavals, plsr_results_2, pheno_fit_residuals_df,\n",
    "                                     vi_detrended_stnd)\n",
    "        #\n",
    "        # save results\n",
    "        save_plsr_results(metavals, plsr_results_3)\n",
    "        #\n",
    "        # Check model AICc\n",
    "        last_round = metavals['outlier_round'] - 1\n",
    "        last_aicc = overall_aicc[-1]\n",
    "        print(f'Round {last_round} final AICc: {last_aicc:.1f}')\n",
    "        this_round = metavals['outlier_round']\n",
    "        this_aicc = plsr_results_3[f'{vi_name}_round_{str(metavals[\"outlier_round\"])}_final_aicc']\n",
    "        print(f'Round {this_round} final AICc: {this_aicc:.1f}')\n",
    "        #\n",
    "        # compare press values and exit iterations if beyond the minimum\n",
    "        if this_aicc < overall_aicc[-1]:\n",
    "            iterate = True\n",
    "            print('- continuing outlier iterations')\n",
    "            overall_aicc.append(this_aicc)\n",
    "        else:\n",
    "            iterate = False\n",
    "            print('- ending outlier iterations')\n",
    "            print()\n",
    "            break\n",
    "        print()\n",
    "        #\n",
    "        # PLS Prediction on image dates\n",
    "        pheno_fit_residuals_df = plsp_image_dates(metavals, plsr_results_3, pheno_fit_params_df,\n",
    "                                                  pheno_fit_residuals_df, trend_params_df, stnd_params_df)\n",
    "        #\n",
    "        # If max_outlier_rounds reached\n",
    "        if outlier_round >= max_outlier_rounds:\n",
    "            iterate = False\n",
    "            print('max_outlier_rounds reached - ending outlier iterations')\n",
    "            print()\n",
    "            break\n",
    "        #\n",
    "        # Check for outliers\n",
    "        outlier_dates = find_outliers(metavals, pheno_fit_residuals_df)\n",
    "        print()\n",
    "        print(f'End of Round {this_round}: post-PLS outlier identification')\n",
    "        if outlier_dates:\n",
    "            confidence = metavals['outlier_confidence']\n",
    "            confidence_str = f'{(confidence * 100):.1f}%'\n",
    "            print(f'- found {len(outlier_dates)} outlier dates based on {confidence_str} prediction interval')\n",
    "            print(f'{str(outlier_dates)}')\n",
    "            print()\n",
    "        else:\n",
    "            print('no more outliers found - ending outlier iterations')\n",
    "            iterate = False\n",
    "            outfname = f'{point_id}_outliers_{vi_name}_{curve_str}_{x_basis}_round_{this_round}.csv'\n",
    "            outfpath = f'{output_path}/{outfname}'\n",
    "            wxcd_vi_outliers_df.to_csv(outfpath, index=False)\n",
    "            print(f'- saved {outfname}')            \n",
    "            print()\n",
    "            break\n",
    "    if graph_output:\n",
    "        plot_plsr_summary(metavals, plsr_results_3)\n",
    "    #\n",
    "    min_aicc = np.min(overall_aicc)\n",
    "    optimal_round = np.argmin(overall_aicc)\n",
    "    metavals['optimal_outlier_round'] = optimal_round\n",
    "    print(f'Outlier iterations optimized at {optimal_round} rounds')\n",
    "    print(f'- minimum AICc = {min_aicc:.1f}')\n",
    "    #\n",
    "    outfname = f'{point_id}_metavals_{vi_name}_{curve_str}_{x_basis}.csv'\n",
    "    outfpath = f'{output_path}/{outfname}'\n",
    "    with open(outfpath, 'w') as outfile:\n",
    "        w = csv.writer(outfile)\n",
    "        for key, val in metavals.items():\n",
    "            w.writerow([key, val])\n",
    "    print(f'- saved {outfname}')\n",
    "    print()\n",
    "    #\n",
    "    return\n",
    "\n",
    "\n",
    "#\n",
    "# end contents of phenology_model_remove_outliers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 60**: invoke the main model looping function, if outlier removal is indicated\n",
    "\n",
    "Calls function in **Notebook Cell 59**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of model_vi_phenology.py\n",
    "#\n",
    "\n",
    "\n",
    "if remove_outliers and outlier_dates:\n",
    "    remove_outliers(metavals, outlier_dates, plsr_results, wxcd_vi_df)\n",
    "\n",
    "\n",
    "#\n",
    "# end part of model_vi_phenology.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model plotting + reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 61**: report the final mean phenology curve, its metrics, and the VI outlier observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "\n",
    "\n",
    "def plot_vi_values_fitted_curve_outliers(vi_name, vi_doys, vi_values, doys_range,\n",
    "                                         season_start, season_end, begin_year, end_year,\n",
    "                                         full_year_mean_vi, var_fit_params, outlier_round,\n",
    "                                         rsq, output_path, fname_tuple, vi_outliers_df):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(vi_doys, vi_values, marker='x', c='k', s=30)\n",
    "    outliers_doys = np.array(vi_outliers_df['DOY'])\n",
    "    outliers_values = np.array(vi_outliers_df[vi_name])\n",
    "    plt.scatter(outliers_doys, outliers_values, marker='o', c='r',\n",
    "                edgecolor='k', s=30)\n",
    "    plt.plot(doys_range, full_year_mean_vi, 'b-', linewidth=2)\n",
    "    control_doys = [var_fit_params[2], var_fit_params[6],\n",
    "                    var_fit_params[10], var_fit_params[12]]\n",
    "    control_vi_values = [full_year_mean_vi[int(round(var_fit_params[2])) - 1],\n",
    "                         var_fit_params[4], var_fit_params[8], \n",
    "                         full_year_mean_vi[int(round(var_fit_params[12])) - 1]]\n",
    "    plt.scatter(control_doys, control_vi_values, marker='o', c='g', edgecolor='g', s=50)\n",
    "    plt.xlim([season_start, season_end])\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel('DOY', fontsize=11)\n",
    "    plt.ylabel(vi_name, fontsize=11)\n",
    "    title = f'{begin_year}-{end_year} {vi_name} mean phenocurve '\n",
    "    title += f'(n = {len(vi_values)}, Rsq = {rsq:.3f}, o = {len(outliers_values)})'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plotfname = '%s_%s_%s_%s_final_mean_fit+outliers.png' % fname_tuple\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    #\n",
    "    return\n",
    "\n",
    "\n",
    "print('Post-modeling summary')\n",
    "print('Part 1: Mean phenology')\n",
    "print()\n",
    "#\n",
    "# pull needed metavals\n",
    "output_path = metavals['output_path']\n",
    "point_id = metavals['point_id']\n",
    "vi_name = metavals['vi_name']\n",
    "curve_str = metavals['vi_curve_str']\n",
    "x_basis = metavals['x_basis']\n",
    "curve_fit_vars = abc_curve_fit_vars\n",
    "begin_year = metavals['analysis_begin_year']\n",
    "end_year = metavals['analysis_end_year']\n",
    "outlier_round = metavals['outlier_round']\n",
    "remove_outliers = metavals['remove_outliers']\n",
    "n_outliers = metavals['n_outliers']\n",
    "doy_min = metavals['doy_min']\n",
    "doy_max = metavals['doy_max']\n",
    "growing_season_start = metavals['growing_season_start']\n",
    "growing_season_end = metavals['growing_season_end']\n",
    "optimal_outlier_round = metavals['optimal_outlier_round']\n",
    "graph_output = metavals['graph_output']\n",
    "fname_tuple = (point_id, vi_name, curve_str, x_basis)\n",
    "#\n",
    "# get original input dataset\n",
    "full_dataset_fname = f'{point_id}_input_WxCD_VI_cleaned_trimmed.csv'\n",
    "full_dataset_fpath = f'{output_path}/{full_dataset_fname}'\n",
    "print(f'reading {full_dataset_fname}')\n",
    "wxcd_vi_all_df = pd.read_csv(full_dataset_fpath, index_col=None)\n",
    "#\n",
    "# get outliers\n",
    "for outlier_round in range(optimal_outlier_round):\n",
    "    outliers_fname = f'{point_id}_outliers_{vi_name}_{curve_str}_{x_basis}_round_{outlier_round}.csv'\n",
    "    outliers_fpath = f'{output_path}/{outliers_fname}'\n",
    "    print(f'reading {outliers_fname}')\n",
    "    if not outlier_round:\n",
    "        vi_outliers_df = pd.read_csv(outliers_fpath, index_col=None)\n",
    "    else:\n",
    "        vi_outliers_df = pd.concat([vi_outliers_df, pd.read_csv(outliers_fpath, index_col=None)], axis=0)\n",
    "outliers_fname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_all_outliers.csv'\n",
    "outliers_fpath = f'{output_path}/{outliers_fname}'\n",
    "vi_outliers_df.to_csv(outliers_fpath, index=False)\n",
    "print(f'saved {outliers_fname}')\n",
    "#\n",
    "# split dataset to retained and outlier dates\n",
    "outlier_dates = list(vi_outliers_df['Date'])\n",
    "wxcd_vi_retained_df = wxcd_vi_all_df[~wxcd_vi_all_df.Date.isin(outlier_dates)]\n",
    "wxcd_vi_outliers_df = wxcd_vi_all_df[wxcd_vi_all_df.Date.isin(outlier_dates)]\n",
    "vi_doys = np.array(wxcd_vi_retained_df[x_basis]).astype(int)\n",
    "vi_values = np.array(wxcd_vi_retained_df[vi_name])\n",
    "#\n",
    "# get full-year mean phenology curve\n",
    "infname = f'{point_id}_retained_{vi_name}_{curve_str}_{x_basis}_round_{optimal_outlier_round}_full_year.npy'\n",
    "infpath = f'{output_path}/{infname}'\n",
    "outfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_mean_phenology_full_year.npy'\n",
    "outfpath = f'{output_path}/{outfname}'\n",
    "os.system(f'cp {infpath} {outfpath}')\n",
    "print(f'reading {outfname}')\n",
    "full_year_mean_vi = np.load(outfpath)\n",
    "#\n",
    "# get mean phenology curve fit parameters\n",
    "infname = f'{point_id}_retained_{vi_name}_{curve_str}_{x_basis}_round_{optimal_outlier_round}_fit_params.csv'\n",
    "infpath = f'{output_path}/{infname}'\n",
    "outfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_mean_phenology_fit_params.csv'\n",
    "outfpath = f'{output_path}/{outfname}'\n",
    "os.system(f'cp {infpath} {outfpath}')\n",
    "print(f'reading {outfname}')\n",
    "pheno_fit_params_df = pd.read_csv(outfpath, index_col=None)\n",
    "vi_fit_params = pheno_fit_params_df[f'{vi_name}_by_{x_basis}']\n",
    "rsq = vi_fit_params[curve_fit_vars.index('Rsq')]\n",
    "#\n",
    "doys_range = np.arange(doy_min, doy_max + 1)\n",
    "if graph_output:\n",
    "    plot_vi_values_fitted_curve_outliers(vi_name, vi_doys, vi_values, doys_range,\n",
    "                                         growing_season_start, growing_season_end,\n",
    "                                         begin_year, end_year, full_year_mean_vi,\n",
    "                                         vi_fit_params, outlier_round, rsq,\n",
    "                                         output_path, fname_tuple, wxcd_vi_outliers_df)\n",
    "#\n",
    "# get mean phenology curve fit metrics\n",
    "infname = f'{point_id}_retained_{vi_name}_{curve_str}_{x_basis}_round_{optimal_outlier_round}_fit_metrics.csv'\n",
    "infpath = f'{output_path}/{infname}'\n",
    "outfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_mean_phenology_fit_metrics.csv'\n",
    "outfpath = f'{output_path}/{outfname}'\n",
    "os.system(f'cp {infpath} {outfpath}')\n",
    "print(f'reading {outfname}')\n",
    "pheno_fit_metrics_df = pd.read_csv(outfpath, index_col=None)\n",
    "print()\n",
    "#\n",
    "print('Mean phenology curve metrics')\n",
    "print(pheno_fit_metrics_df)\n",
    "print()\n",
    "\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 62**: report the final PLS model variables and their VIP and beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# \n",
    "\n",
    "\n",
    "print('Post-modeling summary')\n",
    "print('Part 2: PLS regression results')\n",
    "print()\n",
    "#\n",
    "# pull needed metavals\n",
    "output_path = metavals['output_path']\n",
    "point_id = metavals['point_id']\n",
    "vi_name = metavals['vi_name']\n",
    "curve_str = metavals['vi_curve_str']\n",
    "x_basis = metavals['x_basis']\n",
    "optimal_outlier_round = metavals['optimal_outlier_round']\n",
    "#\n",
    "# find final PLSR results\n",
    "infname = f'{point_id}_pls_retained_{vi_name}_{curve_str}_{x_basis}_round_{optimal_outlier_round}_results.csv'\n",
    "infpath = f'{output_path}/{infname}'\n",
    "print(f'reading {infname}')\n",
    "summary_data_df = pd.read_csv(infpath, index_col=None)\n",
    "summary_data_df.columns = ['key', 'value']\n",
    "var_data_df = \\\n",
    "    summary_data_df[summary_data_df['key'].str.contains(f'{vi_name}_round_{optimal_outlier_round}_final_')]\n",
    "outfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_PLSR_results.csv'\n",
    "outfpath = f'{output_path}/{outfname}'\n",
    "var_data_df.to_csv(outfpath, index=False)\n",
    "print(f'wrote {outfname}')\n",
    "#\n",
    "# extract relevant PLSR results\n",
    "regression_vars_str = var_data_df.loc[var_data_df.key==f'{vi_name}_round_{optimal_outlier_round}_final_vars',\n",
    "                                      'value'].values[0]\n",
    "regression_vars = ast.literal_eval(regression_vars_str)\n",
    "regression_vip_str = var_data_df.loc[var_data_df.key==f'{vi_name}_round_{optimal_outlier_round}_final_vip',\n",
    "                                     'value'].values[0]\n",
    "regression_vip = ast.literal_eval(regression_vip_str)\n",
    "regression_betas_str = var_data_df.loc[var_data_df.key==f'{vi_name}_round_{optimal_outlier_round}_final_betas',\n",
    "                                       'value'].values[0]\n",
    "regression_betas = ast.literal_eval(regression_betas_str)\n",
    "#\n",
    "# store final variables/VIP/betas\n",
    "regression_results_df = pd.DataFrame({'var': regression_vars,\n",
    "                                      'vip': regression_vip,\n",
    "                                      'beta': regression_betas})\n",
    "regression_results_df.sort_values(by=['vip'], ascending=False, inplace=True)\n",
    "regression_results_df.reset_index(drop=True, inplace=True)\n",
    "outfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_PLSR_vars+betas.csv'\n",
    "outfpath = f'{output_path}/{outfname}'\n",
    "regression_results_df.to_csv(outfpath, index=False)\n",
    "print(f'wrote {outfname}')\n",
    "print()\n",
    "#\n",
    "print('PLS regression variables and beta coefficients')\n",
    "print(regression_results_df)\n",
    "print()\n",
    "\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Cell 63**: report the final PLS-based predicted VI values and observation anomalies, and plot time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "\n",
    "\n",
    "def pls_prediction_regression_plot(metavals, prediction_all_df):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    x_basis = metavals['x_basis']\n",
    "    #\n",
    "    all_observed = prediction_all_df[vi_name]\n",
    "    all_predicted = prediction_all_df[f'{vi_name}_predicted']\n",
    "    retained_dates_df = prediction_all_df[prediction_all_df[f'{vi_name}_outlier'] == 0]\n",
    "    outlier_dates_df = prediction_all_df[prediction_all_df[f'{vi_name}_outlier'] == 1]\n",
    "    retained_observed = retained_dates_df[vi_name]\n",
    "    retained_predicted = retained_dates_df[f'{vi_name}_predicted']\n",
    "    outlier_observed = outlier_dates_df[vi_name]\n",
    "    outlier_predicted = outlier_dates_df[f'{vi_name}_predicted']\n",
    "    #\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    #\n",
    "    # plot retained dates\n",
    "    plt.scatter(retained_predicted, retained_observed, marker='x', c='k', s=30,\n",
    "                label=f'modeled dates ({len(retained_predicted)})')\n",
    "    #\n",
    "    # plot outlier dates\n",
    "    plt.scatter(outlier_predicted, outlier_observed, marker='o', c='r', edgecolor='k', s=40,\n",
    "                label=f'outlier dates ({len(outlier_predicted)})')\n",
    "    #\n",
    "    # plot 1:1 line\n",
    "    min_val = np.min([np.min(all_predicted), np.min(all_observed)])\n",
    "    max_val = np.max([np.max(all_predicted), np.max(all_observed)])\n",
    "    plt.plot([min_val-0.02, max_val+0.02], [min_val-0.02, max_val+0.02], \n",
    "             'k--', linewidth=1, label='1:1 line')\n",
    "    #\n",
    "    # plot regression line\n",
    "    regression_stats = regress(retained_predicted, retained_observed)\n",
    "    rsq = regression_stats[2]**2\n",
    "    x_regression = np.arange(np.round(min_val, 2), np.round(max_val, 2) + 0.01, 0.01)\n",
    "    y_regression = regression_stats[0] * x_regression + regression_stats[1]\n",
    "    rsq_str = r'$r^2$'\n",
    "    plt.plot(x_regression, y_regression, 'b-', linewidth=2,\n",
    "             label=f'linear regression')\n",
    "    #\n",
    "    # overplot retained dates\n",
    "    plt.scatter(retained_predicted, retained_observed, marker='x', c='k', s=30)\n",
    "    #\n",
    "    plt.legend(loc='upper left')\n",
    "    #\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(f'PLS-predicted {vi_name}', fontsize=11)\n",
    "    plt.ylabel(f'Observed {vi_name}', fontsize=11)\n",
    "    title = f'Final {vi_name} predictions by PLS ({rsq_str} = {rsq:.3f})'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    #\n",
    "    plotfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_PLS_prediction.png'\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    #\n",
    "    return\n",
    "\n",
    "\n",
    "def pls_prediction_time_series_plot(metavals, prediction_all_df):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    x_basis = metavals['x_basis']\n",
    "    #\n",
    "    all_dyear = prediction_all_df['DYear']\n",
    "    retained_dates_df = prediction_all_df[prediction_all_df[f'{vi_name}_outlier'] == 0]\n",
    "    outlier_dates_df = prediction_all_df[prediction_all_df[f'{vi_name}_outlier'] == 1]\n",
    "    retained_dyear = retained_dates_df['DYear']\n",
    "    retained_observed = retained_dates_df[vi_name]\n",
    "    outlier_dyear = outlier_dates_df['DYear']\n",
    "    outlier_observed = outlier_dates_df[vi_name]\n",
    "    outlier_predicted = outlier_dates_df[f'{vi_name}_predicted']\n",
    "    #\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    #\n",
    "    # plot retained dates\n",
    "    plt.scatter(retained_dyear, retained_observed, marker='x', c='k', s=30,\n",
    "                label=f'modeled observations ({len(retained_dyear)})')\n",
    "    #\n",
    "    # plot outlier observations\n",
    "    plt.scatter(outlier_dyear, outlier_observed, marker='o', c='r', edgecolor='k', s=40,\n",
    "                label=f'outlier observations ({len(outlier_dyear)})')\n",
    "    #\n",
    "    # plot outlier predictions\n",
    "    plt.scatter(outlier_dyear, outlier_predicted, marker='x', c='b', s=30,\n",
    "                label=f'outlier predictions ({len(outlier_dyear)})')\n",
    "    #\n",
    "    # plot zero line\n",
    "    min_val = np.floor(np.min(all_dyear))\n",
    "    max_val = np.ceil(np.max(all_dyear))\n",
    "    plt.plot([min_val, max_val], [0.0, 0.0], 'k--', linewidth=1)\n",
    "    #\n",
    "    plt.legend(loc='upper left')\n",
    "    #\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(f'Date', fontsize=11)\n",
    "    plt.ylabel(vi_name, fontsize=11)\n",
    "    title = f'Landsat {vi_name} observation and prediction time series'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    #\n",
    "    plotfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_time_series.png'\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    return\n",
    "\n",
    "\n",
    "def pls_prediction_time_series_anomaly_plot(metavals, prediction_all_df):\n",
    "    #\n",
    "    # pull needed metavals\n",
    "    output_path = metavals['output_path']\n",
    "    point_id = metavals['point_id']\n",
    "    vi_name = metavals['vi_name']\n",
    "    curve_str = metavals['vi_curve_str']\n",
    "    x_basis = metavals['x_basis']\n",
    "    #\n",
    "    all_dyear = prediction_all_df['DYear']\n",
    "    retained_dates_df = prediction_all_df[prediction_all_df[f'{vi_name}_outlier'] == 0]\n",
    "    outlier_dates_df = prediction_all_df[prediction_all_df[f'{vi_name}_outlier'] == 1]\n",
    "    retained_dyear = retained_dates_df['DYear']\n",
    "    retained_observed = retained_dates_df[f'{vi_name}_predicted_anomaly']\n",
    "    outlier_dyear = outlier_dates_df['DYear']\n",
    "    outlier_observed = outlier_dates_df[f'{vi_name}_predicted_anomaly']\n",
    "    #\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    #\n",
    "    # plot retained dates\n",
    "    plt.scatter(retained_dyear, retained_observed, marker='x', c='k', s=30,\n",
    "                label=f'modeled observations ({len(retained_dyear)})')\n",
    "    #\n",
    "    # plot outlier observations\n",
    "    plt.scatter(outlier_dyear, outlier_observed, marker='o', c='r', edgecolor='k', s=40,\n",
    "                label=f'outlier observations ({len(outlier_dyear)})')\n",
    "    #\n",
    "    # plot zero line\n",
    "    min_val = np.floor(np.min(all_dyear))\n",
    "    max_val = np.ceil(np.max(all_dyear))\n",
    "    plt.plot([min_val, max_val], [0.0, 0.0], 'k--', linewidth=1)\n",
    "    #\n",
    "    plt.legend(loc='upper left')\n",
    "    #\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(f'Date', fontsize=11)\n",
    "    plt.ylabel(f'predicted {vi_name} anomaly', fontsize=11)\n",
    "    title = f'Landsat {vi_name} predicted anomaly time series'\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    #\n",
    "    plotfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_time_series_anomalies.png'\n",
    "    plotfpath = f'{output_path}/{plotfname}'\n",
    "    plt.savefig(plotfpath, dpi=300, bbox_inches='tight')\n",
    "    print(f'saved {plotfname}')\n",
    "    return\n",
    "\n",
    "\n",
    "print('Post-modeling summary')\n",
    "print('Part 3: PLS prediction on original data')\n",
    "print()\n",
    "#\n",
    "# pull needed metavals\n",
    "output_path = metavals['output_path']\n",
    "point_id = metavals['point_id']\n",
    "vi_name = metavals['vi_name']\n",
    "curve_str = metavals['vi_curve_str']\n",
    "x_basis = metavals['x_basis']\n",
    "optimal_outlier_round = metavals['optimal_outlier_round']\n",
    "graph_output = metavals['graph_output']\n",
    "#\n",
    "# get original input dataset\n",
    "full_dataset_fname = f'{point_id}_input_WxCD_VI_cleaned_trimmed.csv'\n",
    "full_dataset_fpath = f'{output_path}/{full_dataset_fname}'\n",
    "print(f'reading {full_dataset_fname}')\n",
    "wxcd_vi_all_df = pd.read_csv(full_dataset_fpath, index_col=None)\n",
    "all_dates = np.array(wxcd_vi_all_df['Date'])\n",
    "all_doys = np.array(wxcd_vi_all_df['DOY'])\n",
    "all_dyears = np.array(wxcd_vi_all_df['DYear'])\n",
    "all_vi_values = np.array(wxcd_vi_all_df[vi_name])\n",
    "n_all_dates = len(all_dates)\n",
    "#\n",
    "# get final detrending parameters for all variables\n",
    "infname = f'{point_id}_pls_retained_{vi_name}_{curve_str}_{x_basis}_round_{optimal_outlier_round}_trend_params.csv'\n",
    "infpath = f'{output_path}/{infname}'\n",
    "outfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_trend_params.csv'\n",
    "outfpath = f'{output_path}/{outfname}'\n",
    "os.system(f'cp {infpath} {outfpath}')\n",
    "print(f'reading {outfname}')\n",
    "trend_params_df = pd.read_csv(outfpath, index_col=None)\n",
    "#\n",
    "# get final standardizing parameters for all variables\n",
    "infname = f'{point_id}_pls_retained_{vi_name}_{curve_str}_{x_basis}_round_{optimal_outlier_round}_stnd_params.csv'\n",
    "infpath = f'{output_path}/{infname}'\n",
    "outfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_stnd_params.csv'\n",
    "outfpath = f'{output_path}/{outfname}'\n",
    "os.system(f'cp {infpath} {outfpath}')\n",
    "print(f'reading {outfname}')\n",
    "stnd_params_df = pd.read_csv(outfpath, index_col=None)\n",
    "#\n",
    "# get final set of outlier dates\n",
    "outliers_fname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_all_outliers.csv'\n",
    "outliers_fpath = f'{output_path}/{outliers_fname}'\n",
    "print(f'reading {outliers_fname}')\n",
    "vi_outliers_df = pd.read_csv(outliers_fpath, index_col=None)\n",
    "outlier_dates = list(vi_outliers_df['Date'])\n",
    "#\n",
    "# get final n_components\n",
    "infname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_PLSR_results.csv'\n",
    "infpath = f'{output_path}/{infname}'\n",
    "print(f'reading {infname}')\n",
    "var_data_df = pd.read_csv(infpath, index_col=None)\n",
    "regression_n_components = \\\n",
    "    var_data_df.loc[var_data_df.key==f'{vi_name}_round_{optimal_outlier_round}_final_n_components',\n",
    "                    'value'].values[0]\n",
    "#\n",
    "# get final variables and betas\n",
    "infname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_PLSR_vars+betas.csv'\n",
    "infpath = f'{output_path}/{infname}'\n",
    "print(f'reading {infname}')\n",
    "var_data_df = pd.read_csv(infpath, index_col=None)\n",
    "regression_vars = list(var_data_df['var'])\n",
    "regression_nvars = len(regression_vars)\n",
    "regression_betas = np.array(var_data_df['beta'])\n",
    "print()\n",
    "#\n",
    "print(f'Predicting {vi_name} on all original image dates using final PLS model')\n",
    "print(f'- {n_all_dates} dates')\n",
    "print(f'- {regression_nvars} parameters')\n",
    "print(f'- {regression_n_components} components')\n",
    "print()\n",
    "#\n",
    "pls_prediction_df = pd.DataFrame({'Date': all_dates,\n",
    "                                  'DOY': all_doys,\n",
    "                                  'DYear': all_dyears,\n",
    "                                  vi_name: all_vi_values})\n",
    "for var_name in regression_vars:\n",
    "    var_std_anoms = np.array(wxcd_vi_all_df[var_name])\n",
    "    pls_prediction_df[var_name] = var_std_anoms\n",
    "#\n",
    "# add VI mean curve values and calculate residuals\n",
    "all_vi_mean = [full_year_mean_vi[doy-1] for doy in all_doys]\n",
    "pls_prediction_df[f'{vi_name}_fitted'] = all_vi_mean\n",
    "all_vi_residuals = all_vi_values - all_vi_mean\n",
    "pls_prediction_df[f'{vi_name}_residual'] = all_vi_residuals\n",
    "#\n",
    "#\n",
    "regression_wxcd_detrended_stnd = np.zeros((n_all_dates, regression_nvars))\n",
    "for n, var_name in enumerate(regression_vars):\n",
    "    var_values = np.array(wxcd_vi_all_df[var_name])\n",
    "    var_trend_params = np.array(trend_params_df[var_name][:2])\n",
    "    var_detrended = detrend_given_stats(all_dyears, var_values, var_trend_params)\n",
    "    var_stnd_params = np.array(stnd_params_df[f'{var_name}_detrended'])\n",
    "    var_detrended_stnd = standardize_given_stats(var_detrended, var_stnd_params)\n",
    "    regression_wxcd_detrended_stnd[:, n] = var_detrended_stnd\n",
    "    pls_prediction_df[f'{var_name}_detrended_stnd'] = var_detrended_stnd\n",
    "#\n",
    "# calculate predicted VI residuals\n",
    "predicted_vi_residuals_detrended_stnd = np.dot(regression_wxcd_detrended_stnd, regression_betas)\n",
    "#\n",
    "# destandardize + retrend predicted VI residuals\n",
    "vi_residual_stnd_params = np.array(stnd_params_df[f'{vi_name}_residual_detrended'])\n",
    "predicted_vi_residuals_detrended = \\\n",
    "    destandardize(predicted_vi_residuals_detrended_stnd, vi_residual_stnd_params)\n",
    "vi_residual_trend_params = np.array(trend_params_df[f'{vi_name}_residual'][:2])\n",
    "predicted_vi_residuals = \\\n",
    "    retrend(all_dyears, predicted_vi_residuals_detrended, vi_residual_trend_params)\n",
    "pls_prediction_df[f'{vi_name}_residual_predicted'] = predicted_vi_residuals\n",
    "predicted_vi_values = predicted_vi_residuals + all_vi_mean\n",
    "pls_prediction_df[f'{vi_name}_predicted'] = predicted_vi_values\n",
    "predicted_vi_anomalies = all_vi_values - predicted_vi_values\n",
    "pls_prediction_df[f'{vi_name}_predicted_anomaly'] = predicted_vi_anomalies\n",
    "outlier_bool = [int(date in outlier_dates) for date in all_dates]\n",
    "pls_prediction_df[f'{vi_name}_outlier'] = outlier_bool\n",
    "#\n",
    "outfname = f'{point_id}_{vi_name}_{curve_str}_{x_basis}_final_PLS_prediction.csv'\n",
    "outfpath = f'{output_path}/{outfname}'\n",
    "pls_prediction_df.to_csv(outfpath, index=False)\n",
    "print(f'wrote {outfname}')\n",
    "#\n",
    "if graph_output:\n",
    "    pls_prediction_regression_plot(metavals, pls_prediction_df)\n",
    "    # pls_prediction_time_series_plot(metavals, pls_prediction_df)\n",
    "    pls_prediction_time_series_anomaly_plot(metavals, pls_prediction_df)\n",
    "\n",
    "\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **End of CANOPY-Phenology Model v0.99 in Jupyter Notebook format using Python v3.12**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
